{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 109A/STAT 121A/AC 209A/CSCI E-109A: Homework 6\n",
    "# Reg-Logistic Regression, ROC, and Data Imputation\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2017**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, Rahul Dave, Margo Levine\n",
    "\n",
    "---\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in canvas.\n",
    "- Restart the kernel and run the whole notebook again before you submit. \n",
    "- Do not include your name(s) in the notebook if you are submitting as a group. \n",
    "- If you submit individually and you have worked with someone, please include the name of your [one] partner below. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your partner's name (if you submit separately): Walter Thornton and Dwayne Kennemore\n",
    "\n",
    "Enrollment Status (109A, 121A, 209A, or E109A): E109A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\_collections_abc.py:841: MatplotlibDeprecationWarning: \n",
      "The examples.directory rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2. In the future, examples will be found relative to the 'datapath' directory.\n",
      "  self[key] = other[key]\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\_collections_abc.py:841: MatplotlibDeprecationWarning: \n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "  self[key] = other[key]\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\_collections_abc.py:841: MatplotlibDeprecationWarning: \n",
      "The text.latex.unicode rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2.\n",
      "  self[key] = other[key]\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\_collections_abc.py:841: MatplotlibDeprecationWarning: \n",
      "The verbose.fileo rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "  self[key] = other[key]\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\_collections_abc.py:841: MatplotlibDeprecationWarning: \n",
      "The verbose.level rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "  self[key] = other[key]\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\seaborn\\apionly.py:9: UserWarning: As seaborn no longer sets a default style on import, the seaborn.apionly module is deprecated. It will be removed in a future version.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "import seaborn.apionly as sns\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "def leppard(source_data, prediction_data):\n",
    "    false_negative = 0\n",
    "    false_positive = 0\n",
    "    correct_assessment = 0\n",
    "    for result in range(0, len(prediction_data)):\n",
    "        if int(prediction_data[result]) == 1 and int(source_data[result]) == 0:\n",
    "            false_positive += 1\n",
    "        if int(prediction_data[result]) == 0 and int(source_data[result]) == 1:\n",
    "            false_negative += 1\n",
    "        if (int(prediction_data[result]) == 1 and int(source_data[result]) == 1) or (int(prediction_data[result]) == 0 and int(source_data[result]) == 0):\n",
    "            correct_assessment += 1\n",
    "    print ()\n",
    "    print (\"False Positives: \", false_positive)\n",
    "    print (\"False Negatives: \", false_negative)\n",
    "    print (\"Correct Assessment: \", correct_assessment)\n",
    "\n",
    "    print (\"Classification Accuracy: \", 1 - (false_positive + false_negative) / len(source_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Breast Cancer Detection\n",
    "\n",
    "In this homework, we will consider the problem of early breast cancer detection from X-ray images. Specifically, given a candidate region of interest (ROI) from an X-ray image of a patient's breast, the goal is to predict if the region corresponds to a malignant tumor (label 1) or is normal (label 0). The training and test data sets for this problem is provided in the file `hw6_dataset.csv`. Each row in these files corresponds to a ROI in a patient's X-ray, with columns 1-117 containing features computed using standard image processing algorithms. The last column contains the class label, and is based on a radiologist's opinion or a biopsy. This data was obtained from the KDD Cup 2008 challenge.\n",
    "\n",
    "The data set contain a total of 69,098 candidate ROIs, of which only 409 are malignant, while the remaining are all normal. \n",
    "\n",
    "*Note*: be careful of reading/treating column names and row names in this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Beyond Classification Accuracy\n",
    "\n",
    "\n",
    "0.  Split the data set into a training set and a testing set.  The training set should be 75% of the original data set, and the testing set 25%.  Use `np.random.seed(9001)`.\n",
    "\n",
    "1. Fit a logistic regression classifier to the training set and report the  accuracy of the classifier on the test set. You should use $L_2$ regularization in logistic regression, with the regularization parameter tuned using cross-validation. \n",
    "    1. How does the fitted model compare with a classifier that predicts 'normal' (label 0) on all patients? \n",
    "    2. Do you think the difference in the classification accuracies are large enough to declare logistic regression as a better classifier than the all 0's classifier? Why or why not?\n",
    "    \n",
    "For applications with imbalanced class labels, in this case when there are many more healthy subjects ($Y=0$) than those with cancer ($Y=1$), the classification accuracy may not be the best metric to evaluate a classifier's performance. As an alternative, we could analyze the confusion table for the classifier. \n",
    "\n",
    "<ol start=\"3\">\n",
    "<li> Compute the confusion table for both the fitted classifier and the classifier that predicts all 0's.</li>\n",
    "<li> Using the entries of the confusion table compute the *true positive rate* and the *true negative rate* for the two classifiers. Explain what these evaluation metrics mean for the specific task of cancer detection. Based on the observed metrics, comment on whether the fitted model is better than the all 0's classifier.</li>\n",
    "<li> What is the *false positive rate* of the fitted classifier, and how is it related to its true positive and true negative rate? Why is a classifier with high false positive rate undesirable for a cancer detection task?</li>\n",
    "</ol>\n",
    "*Hint:* You may use the `metrics.confusion_matrix` function to compute the confusion matrix for a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-3ae9e6f685a0>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-3ae9e6f685a0>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    Logistic classifier applied to Test Set:\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Logistic classifier applied to Test Set:\n",
    "\n",
    "False Positives:  0\n",
    "False Negatives:  1\n",
    "Correct Assessment:  17087\n",
    "Classification Accuracy:  0.9999414794007491\n",
    "[[16984     0]\n",
    " [    1   103]]\n",
    "\n",
    "\n",
    "Classifier that predicts all normal:\n",
    "\n",
    "False Positives:  0\n",
    "False Negatives:  104\n",
    "Correct Assessment:  16984\n",
    "Classification Accuracy:  0.9939138576779026\n",
    "[[16984     0]\n",
    " [  104     0]]\n",
    "In [ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-6b8b6c4d0e62>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-6b8b6c4d0e62>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    The difference between the two models is substantial in light of the number of patients. The fraction of a percent that\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The difference between the two models is substantial in light of the number of patients. The fraction of a percent that \n",
    "the logistic scored higher translates to a number of missclassifications. Further the number of false negatives is \n",
    "decreased from 104 to 1. This is the number of people who tested negative despite having cancer, the costliest error \n",
    "for our purposes. Having a classifier that predicts all people to not have cancer surely has a high accuracy rate given \n",
    "the rarity of cancer, but it does nothing to help find those rare cases that are so critical to find. On the other extreme are\n",
    "false positives. Predicting all positive makes sure that we find all cases of cancer by predicting that everyone has it. This\n",
    "is also useless as it does nothing to screen patients. False positives in cancer can cause patient anxiety and monetary costs for \n",
    "further tests, while false negatives are lethal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data set into a training set and a testing set\n",
    "np.random.seed(9001)\n",
    "df = pd.read_csv('hw6_dataset.csv')\n",
    "msk = np.random.rand(len(df)) < 0.75\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]\n",
    "orig_columns = list(data_train.columns.values)\n",
    "new_columns = []\n",
    "for x in range (len(orig_columns) - 1):\n",
    "    #print(orig_columns[x])\n",
    "    index_of_e = orig_columns[x].index('e')\n",
    "    revised_string = orig_columns[x][:index_of_e + 4]\n",
    "    #print(revised_string)\n",
    "    converted_string = float(revised_string)\n",
    "    new_columns.append(str(converted_string))\n",
    "new_columns.append('Class Label')\n",
    "#print(new_columns)\n",
    "data_train.columns = new_columns\n",
    "data_test.columns = new_columns\n",
    "data_train.head(10)\n",
    "\n",
    "y_train = data_train['Class Label'].values\n",
    "X_train = data_train.values\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "y_test = data_test['Class Label'].values\n",
    "X_test = data_test.values\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1.439999999999999891e-01</th>\n",
       "      <th>-1.429999999999999882e-01</th>\n",
       "      <th>-1.160000000000000059e-01</th>\n",
       "      <th>-1.029999999999999943e-01</th>\n",
       "      <th>2.260000000000000064e-01</th>\n",
       "      <th>2.099999999999999922e-01</th>\n",
       "      <th>-9.799999999999999822e-01</th>\n",
       "      <th>-7.800000000000000266e-01</th>\n",
       "      <th>-4.739999999999999769e-01</th>\n",
       "      <th>-4.470000000000000084e-01</th>\n",
       "      <th>...</th>\n",
       "      <th>9.250000000000000444e-01</th>\n",
       "      <th>5.160000000000000142e-01</th>\n",
       "      <th>3.439999999999999725e-01</th>\n",
       "      <th>9.060000000000000275e-01</th>\n",
       "      <th>-1.129999999999999893e+00</th>\n",
       "      <th>-5.520000000000000462e-01</th>\n",
       "      <th>5.530000000000000471e-01</th>\n",
       "      <th>-4.169999999999999818e-01</th>\n",
       "      <th>2.560000000000000053e-01</th>\n",
       "      <th>0.000000000000000000e+00</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.01100</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.2230</td>\n",
       "      <td>-0.1730</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.284</td>\n",
       "      <td>-0.0522</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.593</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.00785</td>\n",
       "      <td>-0.533</td>\n",
       "      <td>-0.0789</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.0723</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21200</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>-1.150</td>\n",
       "      <td>-1.8100</td>\n",
       "      <td>-1.560</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>-1.200</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.816</td>\n",
       "      <td>1.570</td>\n",
       "      <td>0.39400</td>\n",
       "      <td>1.340</td>\n",
       "      <td>-1.1800</td>\n",
       "      <td>-2.700</td>\n",
       "      <td>-0.926</td>\n",
       "      <td>-2.650</td>\n",
       "      <td>-0.0447</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.21500</td>\n",
       "      <td>-0.184</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.463</td>\n",
       "      <td>-1.0500</td>\n",
       "      <td>-0.941</td>\n",
       "      <td>-0.531</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.37100</td>\n",
       "      <td>0.859</td>\n",
       "      <td>-0.9930</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.326</td>\n",
       "      <td>-0.0528</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.27900</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-1.3200</td>\n",
       "      <td>-0.994</td>\n",
       "      <td>-1.110</td>\n",
       "      <td>-1.090</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.640</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.29500</td>\n",
       "      <td>0.403</td>\n",
       "      <td>-1.1200</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-0.820</td>\n",
       "      <td>0.4350</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00922</td>\n",
       "      <td>-0.138</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>0.1540</td>\n",
       "      <td>-0.391</td>\n",
       "      <td>-0.397</td>\n",
       "      <td>-1.6900</td>\n",
       "      <td>-1.450</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>-0.527</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.37100</td>\n",
       "      <td>0.481</td>\n",
       "      <td>-1.0600</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>0.550</td>\n",
       "      <td>-0.284</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   -1.439999999999999891e-01  -1.429999999999999882e-01  \\\n",
       "0                   -0.01100                      0.138   \n",
       "1                    0.21200                     -0.313   \n",
       "2                    0.21500                     -0.184   \n",
       "3                    0.27900                     -0.197   \n",
       "4                    0.00922                     -0.138   \n",
       "\n",
       "   -1.160000000000000059e-01  -1.029999999999999943e-01  \\\n",
       "0                    -0.2230                    -0.1730   \n",
       "1                     0.2660                     0.2320   \n",
       "2                     0.0274                     0.0494   \n",
       "3                     0.1270                     0.0973   \n",
       "4                     0.1690                     0.1540   \n",
       "\n",
       "   2.260000000000000064e-01  2.099999999999999922e-01  \\\n",
       "0                     0.188                     0.284   \n",
       "1                    -1.190                    -1.150   \n",
       "2                     0.443                     0.463   \n",
       "3                    -0.213                    -0.150   \n",
       "4                    -0.391                    -0.397   \n",
       "\n",
       "   -9.799999999999999822e-01  -7.800000000000000266e-01  \\\n",
       "0                    -0.0522                     -0.256   \n",
       "1                    -1.8100                     -1.560   \n",
       "2                    -1.0500                     -0.941   \n",
       "3                    -1.3200                     -0.994   \n",
       "4                    -1.6900                     -1.450   \n",
       "\n",
       "   -4.739999999999999769e-01  -4.470000000000000084e-01  ...  \\\n",
       "0                      0.129                      0.427  ...   \n",
       "1                     -1.250                     -1.200  ...   \n",
       "2                     -0.531                     -0.394  ...   \n",
       "3                     -1.110                     -1.090  ...   \n",
       "4                     -0.546                     -0.527  ...   \n",
       "\n",
       "   9.250000000000000444e-01  5.160000000000000142e-01  \\\n",
       "0                    -0.593                     0.452   \n",
       "1                    -0.816                     1.570   \n",
       "2                     0.634                     0.111   \n",
       "3                    -0.640                     0.485   \n",
       "4                    -0.277                     0.699   \n",
       "\n",
       "   3.439999999999999725e-01  9.060000000000000275e-01  \\\n",
       "0                   0.00785                    -0.533   \n",
       "1                   0.39400                     1.340   \n",
       "2                   0.37100                     0.859   \n",
       "3                   0.29500                     0.403   \n",
       "4                   0.37100                     0.481   \n",
       "\n",
       "   -1.129999999999999893e+00  -5.520000000000000462e-01  \\\n",
       "0                    -0.0789                      0.705   \n",
       "1                    -1.1800                     -2.700   \n",
       "2                    -0.9930                     -0.492   \n",
       "3                    -1.1200                     -0.343   \n",
       "4                    -1.0600                     -0.526   \n",
       "\n",
       "   5.530000000000000471e-01  -4.169999999999999818e-01  \\\n",
       "0                     0.906                      0.216   \n",
       "1                    -0.926                     -2.650   \n",
       "2                     0.363                      0.326   \n",
       "3                     0.468                     -0.820   \n",
       "4                     0.550                     -0.284   \n",
       "\n",
       "   2.560000000000000053e-01  0.000000000000000000e+00  \n",
       "0                   -0.0723                       0.0  \n",
       "1                   -0.0447                       0.0  \n",
       "2                   -0.0528                       0.0  \n",
       "3                    0.4350                       0.0  \n",
       "4                    0.1550                       0.0  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The optimized L2 regularization paramater id: [10.]\n",
      "Estimated beta1: \n",
      " [[-2.15675716e-02  4.57672300e-02  5.87118917e-01  5.01597813e-01\n",
      "  -2.54420820e-01 -2.18567419e-01  1.68100070e-01 -7.42715978e-02\n",
      "   1.15897490e-01 -3.84389796e-02 -5.24469339e-02  1.31078418e-01\n",
      "  -2.18614609e-01  1.63000901e-02  2.81643854e-02 -1.40624692e-02\n",
      "  -7.47535810e-02 -7.52547740e-02 -1.01655866e-01 -9.82645510e-02\n",
      "   1.57753711e-01  1.90654370e-02  8.81488370e-02 -3.22963552e-02\n",
      "   2.91600410e-01 -5.44620432e-02  2.61948087e-02 -9.40194835e-02\n",
      "  -1.88076500e-02 -3.40947727e-02 -4.57707497e-02  7.21185944e-02\n",
      "   2.34914790e-01  8.33270623e-02  4.96411460e-02 -6.72526506e-02\n",
      "   6.36390894e-02  5.50162344e-02 -1.41451634e-02  1.01859379e-02\n",
      "   1.31628151e-02  1.98233753e-02  2.33209298e-02 -7.50357160e-02\n",
      "   2.06580854e-02  3.89939038e-02  5.63545215e-02 -1.72732065e-01\n",
      "  -1.56078567e-01 -6.21999384e-02 -1.59293855e-01  1.10459113e-01\n",
      "  -4.91734272e-02 -3.82534691e-02 -3.06234683e-02 -2.53074462e-02\n",
      "  -1.69751540e-02  2.64366837e-01 -2.61942751e-01  7.40221189e-02\n",
      "  -5.09557884e-02  7.36473420e-02  7.21481034e-02  8.75476228e-02\n",
      "   9.49586298e-02  1.34815232e-02  2.39635121e-01 -1.65466525e-01\n",
      "  -1.22649065e-02  1.44632429e-02 -3.08337972e-02  3.58424383e-02\n",
      "   1.01931085e-01  1.34155244e-02  1.07915224e-02  8.49930811e-03\n",
      "   3.31735369e-02  8.11659701e-02  1.02843607e-02  7.39264347e-02\n",
      "   7.17942172e-02 -3.67858159e-02 -3.15508875e-02 -6.81644127e-03\n",
      "   3.21578183e-02  3.28756416e-02  6.09449560e-02  1.18699486e-01\n",
      "   1.63958786e-01 -4.67669716e-02 -5.25847321e-02 -5.38052937e-02\n",
      "  -1.15033883e-01 -4.30447503e-02  3.07031935e-02 -2.42748757e-01\n",
      "  -6.27315465e-02 -1.81842225e-02 -1.08988725e-02  1.19280915e-01\n",
      "   1.30252942e-01  6.46796848e-02 -1.83790319e-01 -4.91054600e-02\n",
      "   1.48517001e-02 -1.34156423e-02 -7.81414800e-02  1.96245506e-02\n",
      "   7.85621616e-02 -6.15018861e-02 -8.59113492e-02 -2.00210059e-01\n",
      "  -5.88035786e-02 -7.06945459e-02  8.43177229e-03  7.70495967e-02\n",
      "  -5.77997519e-02  1.12553641e+01]]\n",
      "Estimated beta0: \n",
      " [-9.51671298]\n",
      "\n",
      "\n",
      "malignant:  103.0\n",
      "\n",
      "\n",
      "Classifier applied to Test Set:\n",
      "\n",
      "False Positives:  0\n",
      "False Negatives:  1\n",
      "Correct Assessment:  17087\n",
      "Classification Accuracy:  0.9999414794007491\n",
      "[[16984     0]\n",
      " [    1   103]]\n",
      "\n",
      "\n",
      "Classifier that predicts all normal:\n",
      "\n",
      "False Positives:  0\n",
      "False Negatives:  104\n",
      "Correct Assessment:  16984\n",
      "Classification Accuracy:  0.9939138576779026\n",
      "[[16984     0]\n",
      " [  104     0]]\n"
     ]
    }
   ],
   "source": [
    "# Fit a logistic regression classifier to the training set and report the accuracy of the classifier on the test set\n",
    "clf = LogisticRegressionCV(\n",
    "        Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
    "        ,penalty='l2'\n",
    "        ,cv=10\n",
    "        ,random_state=777\n",
    "        ,fit_intercept=True\n",
    "        ,solver='newton-cg'\n",
    "        ,tol=10)\n",
    "clf.fit(X_train, y_train)\n",
    "print('\\n')\n",
    "print(\"The optimized L2 regularization paramater id:\", clf.C_)\n",
    "\n",
    "# The coefficients\n",
    "print('Estimated beta1: \\n', clf.coef_)\n",
    "print('Estimated beta0: \\n', clf.intercept_)\n",
    "\n",
    "# Scoring\n",
    "clf_y_pred_test = clf.predict(X_test)\n",
    "clf_y_pred_test = clf_y_pred_test.reshape(len(clf_y_pred_test), 1)\n",
    "test_df = pd.DataFrame(clf_y_pred_test)\n",
    "Total = test_df[0].sum()\n",
    "print('\\n')\n",
    "print(\"malignant: \", Total)\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "test_df['All Normal'] = 0\n",
    "\n",
    "# Reset indexes so copy will work\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "data_test = data_test.reset_index(drop=True)\n",
    "test_df['Class Label'] = data_test['Class Label']\n",
    "\n",
    "# Confusion Matrix\n",
    "print('\\n')\n",
    "print('Classifier applied to Test Set:') \n",
    "leppard(test_df['Class Label'], test_df[0])\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('Classifier that predicts all normal:')\n",
    "leppard(test_df['Class Label'], test_df['All Normal'])\n",
    "print(confusion_matrix(y_test, test_df['All Normal']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix that predicts all patients to be negative:\n",
      "[[16984     0]\n",
      " [  104     0]]\n"
     ]
    }
   ],
   "source": [
    "# sanity check, use function from lab to check confusion matrix at threshold level of one.\n",
    "# this matches our previously computed confusion matrix\n",
    "def t_repredict(est, t, xtest):\n",
    "    probs = est.predict_proba(xtest)\n",
    "    p0 = probs[:,0]\n",
    "    p1 = probs[:,1]\n",
    "    ypred = (p1 > t)*1\n",
    "    return ypred\n",
    "print('Confusion matrix that predicts all patients to be negative:')\n",
    "print(confusion_matrix(y_test,t_repredict(clf, 01.00, X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: ROC Analysis\n",
    "\n",
    "Another powerful diagnostic tool for class-imbalanced classification tasks is the Receiver Operating Characteristic (ROC) curve. Notice that the default logistic regression classifier in `sklearn` classifies a data point by thresholding the predicted class probability $\\hat{P}(Y=1)$ at 0.5. By using a different threshold, we can adjust the trade-off between the true positive rate (TPR) and false positive rate (FPR) of the classifier. The ROC curve allows us to visualize this trade-off across all possible thresholds.\n",
    "\n",
    "\n",
    "1. Display the ROC curve for the fitted classifier on the *test set*. In the same plot, also display the ROC curve for the all 0's classifier. How do the two curves compare?\n",
    "\n",
    "2.  Compute the highest TPR that can be achieved by the classifier at each of the following FPR's, and the thresholds at which they are achieved. Based on your results, comment on how the threshold influences a classifier's FPR.\n",
    "    - FPR = 0\n",
    "    - FPR = 0.1\n",
    "    - FPR = 0.5\n",
    "    - FPR = 0.9\n",
    "- Suppose a clinician told you that diagnosing a cancer patient as normal is *twice* as critical an error as diagnosing a normal patient as having cancer. Based on this information, what threshold would you recommend the clinician to use? What is the TPR and FPR of the classifier at this threshold? \n",
    "\n",
    "- Compute the area under the ROC curve (AUC) for both the fitted classifier and the all 0's classifier. How does the difference in the AUCs of the two classifiers compare with the difference between their classification accuracies in Question 1, Part 2(A)? \n",
    "\n",
    "*Hint:* You may use the `metrics.roc_curve` function to compute the ROC curve for a classification model and the `metrics.roc_auc_score` function to compute the AUC for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained above, the logistic model is much better and here is the visualization of that. The curve of the logistic is steep\n",
    "getting us to 1 tpr quickly while taking on few false positives. On the otherhand, our all negative model takes on false positives as quickly as it takes on true positives, not as effective for our purposes. the area under the curve for the logistic\n",
    "is 1 whereas the auc for the all negative model is 1/2. This seems to be a better measure of the models accuracy.  The gap in aucs demonstrates the effective difference in these models better than the classification accuracy score, the scores being so close together +.999. The models are in truth very different and AUC reflects this difference.\n",
    "In computing the roc curve, we ended up with arrays of 11 values. 11 fprs, 11 tprs and 11 thresholds.\n",
    "We searched through these lists for the value that was equal to or larger than the given fprs, 0, .1, .5, and .9\n",
    "Our results printed below are the next highest values of fpr that satisfy that condition. Our curve is very steep and goes all the way to 1 quickly. What this tells us is that a threshold just above zero, gives us a tpr of 1.\n",
    "If we were using this for cancer detection, we would use a low threshold, such as FPR: 0.0314413565709 TPR 1.0 Threshold 0.000580536840768(from below). This takes on very little false positives while simulateously giving us all true positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXyU5dX4/89JAiQhC5AAsmnYZAkkkR13tAruVqu4tIgtamspVlttq9ZatP1Za9UHl1p9qrQ+KnzVqtQHu9AHXEEWQWTfAsgayEb2bc7vj2syDDCZTEImk+W8X6+8yD1zL2duJnPmuq77OreoKsYYY0xdoiIdgDHGmJbNEoUxxpigLFEYY4wJyhKFMcaYoCxRGGOMCcoShTHGmKAsURhjjAnKEoUxQYjIThEpE5FiETkgInNFJMHv+TNF5P9EpEhECkXk7yIy/Lh9JInI0yKy27ufbd7l1OZ/RcY0nCUKY+p3haomAFnAGcAvAERkIvAv4D2gN9Af+BL4VEQGeNfpCPwHSAemAEnAmUAuMK55X4YxjSM2M9uYuonITmCGqi7yLj8OpKvqZSLyMfCVqt553DYfAIdUdZqIzAB+AwxU1eJmDt+YJmEtCmNCJCJ9gUuAbSISj2sZvBlg1f8HXOT9/RvAPyxJmNbMEoUx9XtXRIqAr4Ec4FdAN9zfz/4A6+8HascfUupYx5hWwxKFMfW7WlUTgfOBobgkkA94gF4B1u8FHPb+nlvHOsa0GpYojAmRqn4IzAWeUNUSYClwXYBVr8cNYAMsAiaLSOdmCdKYMLBEYUzDPA1cJCJZwM+BW0RklogkikhXEXkUmAj82rv+q7guq7dFZKiIRIlIiojcLyKXRuYlGNMwliiMaQBVPQT8Ffilqn4CTAauwY1D7MJdPnu2qm71rl+BG9DeBPwbOAIsx3Vffd7sL8CYRrDLY40xxgRlLQpjjDFBWaIwxhgTlCUKY4wxQVmiMMYYE1RMpANoqNTUVE1LS4t0GMYY06qsWrXqsKp2b8y2rS5RpKWlsXLlykiHYYwxrYqI7Grsttb1ZIwxJihLFMYYY4KyRGGMMSYoSxTGGGOCskRhjDEmKEsUxhhjggpbohCRl0UkR0TW1fG8iMgcEdkmImtFZFS4YjHGGNN44WxRzAWmBHn+EmCw9+d24I+h7LSi2kNhaVW96xWWVpF9uCSkdcOtJcViTKjsfduGzkHxYeJi6NTYzcM24U5VPxKRtCCrXAX8VV2d82Ui0kVEeqlq0PsL55VU8vySrZw5KJWEToHDL66o5rNth/EoRAlB1w23lhSLMaGy9+3Rc9AxOopeXeK4bGRvkuM7RDqshju0GVa+TFInSWrsLiI5RtEHd+evWnu8j51ARG4XkZUisrK8rBSPwpGyujP8kbIqPAopCR3rXTfcWlIsxoTK3rdHz0FqYiw1quSVVkY6pIapLIU9q2DXp0AUVR5t9AuI5FcECfBYwLsoqeqLwIsAfQaP0P6pnTl3cI86s/ug7lUUlVdTo0pSbIeg64ZbS4rFmFDZ+/bYcxAtQrf4jpEOKTQeD+RnQ+52t9x7NCh0iJJGv4Cw3uHO2/X0vqqOCPDcn4AlqvqGd3kzcH59XU+D0zN15YqV9b5pC0uryCutpFt8x4i/wVtSLMaEyt63rfAclObBwfVQWQwJPaDHcOgQB2X5xCd1W1dapSMbs9tItigWADNFZB4wHiisL0kAdIiWkP7DkuM7tJj/2JYUizGhsvdtKzoH1ZVweDMU7oGYWOg9ChJ7Hn0+ritl1VQ0dvdhSxQi8gZwPpAqInuAXwEdAFT1BWAhcCmwDSgFbg1XLMYY02YV7oFDm6CmGrr2h9TBEBXdpIcI51VPN9bzvAI/DNfxjTGmTasodt1MZXkQ19V1M8U2+sKmoNrX9W7GGNPaeWrcQHV+Nkg09BwByX1BAl0f1DQsURhjTGtRfAhy1kNVGST1hu5DIabR8+hCZonCGGNauqpyOLQRig5Ax87Qdxx0Tmm2w1uiMMaYlkoVCnbB4a2gHkgZDN0GQFTzzpW2RGGMMS1ReSEcWAcVRyA+FXoOd62JCLBEYYwxLUlNlWtBFOyG6A7QKwuSekU0JEsUxhjTUhzZ78Yiqiugy6mQerpLFhFmicIYYyKtshRyNkDJIeiU5GZWx3WJdFQ+liiMMSZSfAX8toFEuctdu6aFdU5EY1iiMMaYSCjNg4ProLIEEnp6C/jFRjqqgCxRGGNMc6qudLWZjux1lV37jHaVXlswSxTGGNMcVL0F/DaDp9rNh0gZ1OQF/MLBEoUxxoRbRZG3gF++K+DXMx06JUY6qpBZojDGmHDx1LiB6rxsiIpplgJ+4WCJwhhjwqE4x13yWlUGSX28Bfxaye1Uj2OJwhhjmlJVuUsQxQddyY1+4yG+W6SjOimWKIwxpimoQv5O19WkHjerumv/Zi/gFw6WKIwx5mSVFbjB6ooj0Lm7mxPRMT7SUTUZSxTGGNNYNVVweIsr4BfTCXqfAYmnRDqqJmeJwhhjGuPIPsjZ6JJFl9O8Bfza5kdq23xVxhgTLpUlcHADlB52Bfz6jG5RBfzCwRKFMcaEwuOBvB2Qt90V8OsxzLUkWtmciMawRGGMMfUpyYWc9a41kXgKdB/WYgv4hYMlCmOMqUt1hbeA3z5vAb8xkNA90lE1O0sUxhhzPP8CfloD3QZCysBWUcAvHCxRGGOMv/IjbmZ1WT7EdfMW8EuIdFQRZYnCGGPAFfA7vNXNro6OgVNGugJ+xhKFMcZQnONmVleXu+SQOqTVFvALB0sUxpj2q6rMW8AvBzomtIkCfuFgicIY0/7UFvA7vNUtt6ECfuFgicIY076U5XsL+BVB5x5u4lwbKuAXDpYojDHtQ02Vu9y18Os2XcAvHMLazhKRKSKyWUS2icjPAzx/qogsFpHVIrJWRC4NZzzGmHbqyD7I/sjNjeiaBmnnWpJogLC1KEQkGngOuAjYA6wQkQWqusFvtQeB/6eqfxSR4cBCIC1cMRlj2pnKEtfNVJoLscnQd4z71zRIOLuexgHbVHUHgIjMA64C/BOFAkne35OBfWGMxxjTXhxTwC/a3Uioy6ntooBfOIQzUfQBvvZb3gOMP26dh4F/iciPgM7ANwLtSERuB24H6NUvranjNMa0JSWHXSuiqhQSe7nB6phOkY6qVQvnGEWg1K3HLd8IzFXVvsClwKsickJMqvqiqo5R1TFduliz0RgTQHUF7FsDe1a45b5joXeWJYkmEM4WxR6gn99yX07sWvoeMAVAVZeKSCyQCuSEMS5jTFui6q5kOrTFFfBLGQTdBrTbAn7hEM4WxQpgsIj0F5GOwA3AguPW2Q1cCCAiw4BY4FAYYzLGtCXlR2D3MtfVFJsEp50FqYMtSTSxsLUoVLVaRGYC/wSigZdVdb2IzAZWquoC4CfASyJyN65barqqHt89ZYwxx6qphtytkL/LW8AvA5L7RDqqNiusE+5UdSHuklf/xx7y+30DcFY4YzDGtDFFB93d5qorILkfdB8C0R0iHVWbZjOzjTGtg38Bv06J0CvLCvg1E0sUxpiWzeOBgp1weJtb7j4EuqRZAb9mZInCGNNylea5VkRFEST0cBPnOsRFOqp2xxKFMablqamCQ5tcbaaYWOg9ChJ7RjqqdssShTGmZSncC4c2uiubuvZ38yKi7aMqkuzsG2NahopiNx+iLA9iu0DfdDc3wkScJQpjTGR5aiB3O+RnuwJ+PdPdZa9WwK/FCClReGdWn6qq28IcjzGmPbECfq1CvdeXichlwFfAv73LWSLyTrgDM8a0YVXlsG+1XwG/cVbArwULpUUxG1cefDGAqq4RkUFhjcoY0zapQsFuOLwF1AMpg70F/GxOREsWSqKoUtUCOba/0OoxGWMaprzQdTOVF0J8KvQcDh07RzoqE4JQEsVGEbkeiBKR/sBdwLLwhmWMaTOOKeDXAXplQlLvSEdlGiCU9t5MYDTgAf4GlOOShTHGBFd0AHZ+BPk7oUs/6H+uJYlWKJQWxWRV/Rnws9oHROQaXNIwxpgTVZa60hslh1wBv95nQFzXSEdlGimUFsWDAR57oKkDMca0AR6PmxOx8xNXp6n7UHczIUsSrVqdLQoRmYy7TWkfEXnS76kkXDeUMcYcVZrnBqsri62AXxsTrOspB1iHG5NY7/d4EfDzcAZljGlFqivh8OajBfz6jHaJwrQZdSYKVV0NrBaR11S1vBljMsa0FoV7XJXXmmo3HyJlkN2vug0KZTC7j4j8BhgOxNY+qKqnhy0qY0zL5l/AL66r62ayAn5tViiJYi7wKPAEcAlwKzZGYUz7dEIBvxGQ3NcK+LVxoSSKeFX9p4g8oarbgQdF5ONwB2aMaWGKD0HOenfv6qQ+7oqmmI6Rjso0g1ASRYW4+h3bReT7wF7ARqqMaS+qyt2NhIoOuJIb/cZDfLdIR2WaUSiJ4m4gAZgF/AZIBr4bzqCMMS2AKhTsgsNbXQG/1NPdHeesgF+7U2+iUNXPvb8WAd8BEJG+4QzKGBNhZQVusLriiBXwM8EThYiMBfoAn6jqYRFJx5XyuACwZGFMW1NT5VoQBbsguiP0yoKkXpGOykRYnW1IEfn/gNeAm4F/iMgDuHtSfAnYpbHGtDVH9sPOj12S6HKat4CfJQkTvEVxFZCpqmUi0g3Y513e3DyhGWOaxTEF/JKg9yiI6xLpqEwLEixRlKtqGYCq5onIJksSxrQhHo+bD5G7DSTK3a+6y2k2J8KcIFiiGCAitaXEBUjzW0ZVrwlrZMaY8CnNg4ProLIEEnp6C/jF1r+daZeCJYprj1t+NpyBGGOaQXWlq810ZK+r7NpnDCR0j3RUpoULVhTwP80ZiDEmjFS9Bfw2g6caug2ElIFWwM+EJJQJd8aY1qyiyFvAL98V8OuZ7u46Z0yIwjrFUkSmiMhmEdkmIgHvYSEi14vIBhFZLyKvhzMeY9oVT41rQez81N1M6JSRrvyGJQnTQCG3KESkk6pWNGD9aOA54CJgD7BCRBao6ga/dQYDvwDOUtV8EbEaUsY0heIcd8mrFfAzTaDeFoWIjBORr4Ct3uVMEXkmhH2PA7ap6g5VrQTm4eZm+LsNeE5V8wFUNadB0RtjjlVVDnu/gL2r3CWv/cZDrwxLEuakhNKimANcDrwLoKpfisikELbrA3ztt7wHGH/cOqcDiMinQDTwsKr+I4R9G2P8qUL+Tjcnwgr4mSYWSqKIUtVdcuwknJoQtgs0a0cDHH8wcD6udtTHIjJCVQuO2ZHI7cDtAL36pYVwaGPaEf8Cfp27uzkRHeMjHZVpQ0JJFF+LyDhAveMOPwK2hLDdHqCf33JfXBmQ49dZpqpVQLaIbMYljhX+K6nqi8CLAMMzso5PNsa0TzVVcHgLFOyGmE7Q+wxIPCXSUZk2KJR26Q+Ae4BTgYPABO9j9VkBDBaR/iLSEbgBWHDcOu8CkwBEJBXXFbUjtNCNaceO7IPsj6Dga+iaBmnnWpIwYRNKi6JaVW9o6I5VtVpEZgL/xI0/vKyq60VkNrBSVRd4n7tYRDbgurPuVdXchh7LmHajsgQOboDSwxCbDH3HuH+NCSNRDd6TIyLbgc3AfOBvqlrUHIHVZXhGlm5YuyaSIRjT/DweyNsBedvd1Uypg62An2kQEVmlqmMas20od7gbKCJn4rqOfi0ia4B5qjqvMQc0xjRQSS7krHeticRToPswK+BnmlVI186p6meqOgsYBRzB3dDIGBNO1RWw/0vYs9xd8tpnjBuwtiRhmlm9LQoRScBNlLsBGAa8B5wZ5riMab/8C/hpjRXwMxEXymD2OuDvwOOq+nGY4zGmfSs/4uZElBdAXDdvAb+ESEdl2rlQEsUAVfWEPRJj2jNPDRze6mZXR8fAKRmQ3CfSURkDBEkUIvIHVf0J8LaInHBplN3hzpgmUnTQFfCrLofkvpA6xGozmRYlWItivvdfu7OdMeFQVeYSRHEOdExwBfziu0U6KmNOEOwOd8u9vw5T1WOShXcind0Bz5jGUIX8bDi8zS1bAT/TwoXyzvxugMe+19SBGNMulOXDrk/dFU3xKZB2tveKJksSpuUKNkYxFXdJbH8R+ZvfU4lAQeCtjDEB1VS55FD4tbeA3yhI7BnpqIwJSbAxiuVALq7q63N+jxcBq8MZlDFtypF9kLPRJYuuaZAy2F3ZZEwrEWyMIhvIBhY1XzjGtCGVJW5ORGmuFfAzrVqwrqcPVfU8Ecnn2BsOCaCqapdnGBOIx+OK9+XtAIl2NxLqcqoV8DOtVrD2b+3tTlObIxBj2oSSw64VUVUKib2gxzA3JmFMKxas66l2NnY/YJ+qVorI2UAG8D+44oDGGHAF/HI2QtF+6BAPfcdCZ/uOZdqGUK7Jexd3G9SBwF9xhQFfD2tUxrQWqpC/C7I/huKDkDII0s6xJGHalFAuvfCoapWIXAM8rapzRMSuejLGv4BffIobi7ACfqYNCulWqCJyHfAd4GrvYx3CF5IxLVxNNeRudS0JK+Bn2oFQEsV3gTtxZcZ3iEh/4I3whmVMC1V00N1trroCkvtB9yEQbd+bTNsWyq1Q14nILGCQiAwFtqnqb8IfmjEtSFUZHNwAJTnQKdHdaS6ua6SjMqZZhHKHu3OAV4G9uDkUp4jId1T103AHZ0zEeTyugF/udrfcfQh0SbPaTKZdCaXr6SngUlXdACAiw3CJY0w4AzMm4krz3GB1ZTEk9HCD1R3iIh2VMc0ulETRsTZJAKjqRhGxu6qYtqumCg5tcvetjom1An6m3QslUXwhIn/CtSIAbsaKApq2qnAvHNrormzq2t/Ni7ACfqadC+Uv4PvALOA+3BjFR8Az4QzKmGZXUey6mcryILYL9E2H2KRIR2VMixA0UYjISGAg8I6qPt48IRnTjDw1bqA6P9sV8OuZ7i57tQJ+xvgEqx57P+5Odl8AY0Vktqq+3GyRGRNu/gX8knpD96FWwM+YAIK1KG4GMlS1RES6AwsBSxSm9asqd+MQRQe8BfzGQeeUSEdlTIsVLFFUqGoJgKoeEhG7cNy0bqpQsBsObwH1uDvNdRtgcyKMqUewRDHA717ZAgz0v3e2ql4T1siMaUrlhd4CfoUQnwo9h0PHzpGOyphWIViiuPa45WfDGYgxYXFMAb8O0CvTjUcYY0IW7MZF/2nOQIxpckUHIGeDK+DX5VRIPd0K+BnTCDaTyLQ9laUuQZQcsgJ+xjSBsI7iicgUEdksIttE5OdB1vuWiKiIWP0o03gej5sTsfMTV6ep+1A47SxLEsacpJBbFCLSSVUrGrB+NPAccBGwB1ghIgv860Z510vEzfz+PNR9G3OCYwr49YQew6yAnzFNpN4WhYiME5GvgK3e5UwRCaWExzjcvSt2qGolMA+4KsB6jwCPA+Whh22MV3UlHPgKvv4cPNXQZzT0GWVJwpgmFErX0xzgciAXQFW/BCaFsF0f4Gu/5T3ex3xE5Aygn6q+H2xHInK7iKwUkZUFBYUhHNq0C4V7YOdHrpBftwHQ/1xXDtwY06RC6XqKUtVdcmztm5oQtgtULEd9T7oJfE8B0+vbkaq+CLwIMDwjS+tZ3bR1FUXubnNleW78ocdwK+BnTBiFkii+FpFxgHrHHX4EbAlhuz1AP7/lvsA+v+VEYASwxJuETgEWiMiVqroylOBNO1NbwC9vB0TFQM8RkNzXCvgZE2ahJIof4LqfTgUOAou8j9VnBTBYRPrjbqN6A3BT7ZOqWgik1i6LyBLgp5YkTEDFhyBnvbt3dVIfbwE/u3+WMc2h3kShqjm4D/kGUdVqEZkJ/BOIBl5W1fUiMhtYqaoLGhytaX+qyt2ciOKDruRGv/EQ3y3SURnTrtSbKETkJfzGFmqp6u31bauqC3FVZ/0fe6iOdc+vb3+mHVGFgl1weKsr4Jd6urvjnBXwM6bZhdL1tMjv91jgmxx7NZMxTauswM2JqDhiBfyMaQFC6Xqa778sIq8C/w5bRKb9qqlyLYiCXRDdEXplQVKvSEdlTLvXmFpP/YHTmjoQ084d2e9uJlRdAV1Og9TBVsDPmBYilDGKfI6OUUQBeUCddZuMaZDKEsjZ6C3glwS9R0Fcl0hHZYzxEzRRiJvgkIm7vBXAo6o24c2cPI8H8rMhdxtIlKvN1OU0mxNhTAsUNFGoqorIO6o6urkCMu1AaR4cXOdaE4mnQPdh0CE20lEZY+oQyhjFchEZpapfhD0a07ZVV8KhTXBkryva12cMJHSPdFTGmHrUmShEJEZVq4GzgdtEZDtQgqvhpKo6qpliNK2dqivgd2izq/DabSCkDISo6EhHZowJQbAWxXJgFHB1M8Vi2qKKIjcnoizfFfDrme7uOmeMaTWCJQoBUNXtzRSLaUs8NW6gOi8bomPglJGugJ8xptUJlii6i8g9dT2pqk+GIR7TFhTnuFZEdblLDqlDrICfMa1YsEQRDSQQ+L4SxpzomAJ+CVbAz5g2Ilii2K+qs5stEtN6qUL+TtfVpGoF/IxpY+odozAmqLICNyeiogg6d3d3m+sYH+mojDFNKFiiuLDZojCtT00VHN4CBbshphP0PsNNnjPGtDl1JgpVzWvOQEwrcmSfq89UUwVd0yBlsLuyyRjTJtlftwldZQkc3AClhyE2GfqOcf8aY9o0SxSmfh4P5O2AvO3eAn7DocupVsDPmHbCEoUJriTXDVZXlVoBP2PaKUsUJrDqCm8Bv32ugF/fsdA5NdJRGWMiwBKFOZYqFH4Nh7aA1lgBP2OMJQrjp/yIK71RXgBx3bwF/BIiHZUxJsIsURioqXazqvN3egv4ZUByn0hHZYxpISxRtHdFB119ptoCft2HQnSHSEdljGlBLFG0V1Vl3gJ+OVbAzxgTlCWK9sbjgYKdcHibW7YCfsaYeliiaE/K8t1gdUURdO4BPYZZAT9jTL0sUbQHNVXuftWFX3sL+I2CxJ6RjsoY00pYomjrCve6iXNWwM8Y00j2idFWVRS7werSXG8Bv7EQmxTpqIwxrZAlirbG43HF+/J2gES7SXPJ/ayAnzGm0SxRtCUlh91gdVUpJPZyg9UxnSIdlTGmlQvrNZEiMkVENovINhH5eYDn7xGRDSKyVkT+IyKnhTOeNqu6AvatgT0r3HLfsdA7y5KEMaZJhK1FISLRwHPARcAeYIWILFDVDX6rrQbGqGqpiPwAeByYGq6Y2hxVdyvSw1tdAb+UQa6In82JMMY0oXB2PY0DtqnqDgARmQdcBfgShaou9lt/GfDtMMbTtvgX8ItPcTcTsgJ+xpgwCGei6AN87be8BxgfZP3vAR8EekJEbgduB+jVL62Jwmulaqohdyvk73I1mXplQlLvSEdljGnDwpkoAl1mowFXFPk2MAY4L9Dzqvoi8CLA8IysgPtoF4oOQs56NyaR3A+6D7ECfsaYsAtnotgD9PNb7gvsO34lEfkG8ABwnqpWhDGe1quqDA5ugJIc6JQIvc+AuK6RjsoY006EM1GsAAaLSH9gL3ADcJP/CiJyBvAnYIqq5oQxltbJ44H8bMjd7pa7D3EF/GxOhDGmGYUtUahqtYjMBP4JRAMvq+p6EZkNrFTVBcDvgQTgTXEffrtV9cpwxdRUqqqq2LNnD+Xl5eE7iMcDnkp3ZZP0dF1MByvg4KbwHdMY0+rFxsbSt29fOnRoum5pUW1dXf7DM7J0w9o1EY0hOzubxMREUlJSkKb+dq8edxOhmipAoEOcjUMYY0KiquTm5lJUVET//v2PeU5EVqnqmMbs1y64b4Ty8vLwJInqSlejqaYKoju68QhLEsaYEIkIKSkpTd7bYSU8GqlJk4Snxg1Ya42rz9QhDqKim27/xph2o8m/wGKJIrJU3aWuNRWAQEysa0nYYLUxpgWxrqdIqamCymKXJKI6uFnVMZ1CThLR0dFkZWUxYsQIrrjiCgoKCnzPrV+/ngsuuIDTTz+dwYMH88gjj+A/FvXBBx8wZswYhg0bxtChQ/npT3/a5C/vZN14441kZGTw1FNPNWr7uXPnMnPmzEZtu2/fPr71rW/V+XxBQQHPP/98yOsH8uMf/5iPPvqoUfE1hzfffJP09HSioqJYuXJlnev94x//YMiQIQwaNIjHHnvM93h2djbjx49n8ODBTJ06lcrKSgCeffZZXnnllbDHb5qYqraqn2EjMzXSNmzY0OBtCkoqdcehYi0oLletKFEtK1AtP6JaXdWoGDp37uz7fdq0afroo4+qqmppaakOGDBA//nPf6qqaklJiU6ZMkWfffZZVVX96quvdMCAAbpx40ZVVa2qqtLnnnuuUTHUpaqqca+p1v79+/XUU089qWO+8sor+sMf/vCk4qhLdna2pqenN3r73NxcHT9+fIO2Odlz2lAbNmzQTZs26XnnnacrVqwIuE51dbUOGDBAt2/frhUVFZqRkaHr169XVdXrrrtO33jjDVVVveOOO/T5559XVfd+zMrKap4X0Y4F+ozCXW3aqM9da1GcpC0Hi1i1Ky/oz4dbcnh+yVZeX7aD5/+ziQ83H2TV3jJW7a9i1Z4jJ6y/5WBRg2KYOHEie/fuBeD111/nrLPO4uKLLwYgPj6eZ5991vdt7/HHH+eBBx5g6NChAMTExHDnnXeesM/i4mJuvfVWRo4cSUZGBm+//TYACQlH60m99dZbTJ8+HYDp06dzzz33MGnSJO69917S0tKOaeUMGjSIgwcPcujQIa699lrGjh3L2LFj+fTTT0849sUXX0xOTg5ZWVl8/PHHrFmzhgkTJpCRkcE3v/lN8vPzATj//PO5//77Oe+88/iv//qvOs/Prl27uPDCC8nIyODCCy9k9+7dAGzfvp0JEyYwduxYHnroId9r27lzJyNGjABc62zcuHFkZWWRkZHB1q1b+fnPf8727dvJysri3nvvPWb9mpoafvrTn/rO2zPPPHNCPG+99RZTpkzxLc+ePZuxY8cyYsQIbr/9dl/r7/jXV9e5W758OWeeeSZnnHEGZ555Jps3b67zXIRq2LBhDBkyJOg6y5cvZ9CgQQwYMICOHTtyww038N5776Gq/N///Z+vlXXLLbfw7rvvAu79mJaWxvLly086RtN8bIyiGRwprcBTVUlK5w7klno4UrXbAAoAAB0/SURBVNOBhJiOTbLvmpoa/vOf//C9730PcB9so0ePPmadgQMHUlxczJEjR1i3bh0/+clP6t3vI488QnJyMl999RWA78M5mC1btrBo0SKio6PxeDy888473HrrrXz++eekpaXRs2dPbrrpJu6++27OPvtsdu/ezeTJk9m4ceMx+1mwYAGXX345a9a4y6BrP3DPO+88HnroIX7961/z9NNPA64b6MMPPwwa18yZM5k2bRq33HILL7/8MrNmzeLdd9/lrrvu4q677uLGG2/khRdeCLjtCy+8wF133cXNN99MZWUlNTU1PPbYY6xbt84X386dO33rv/jii2RnZ7N69WpiYmLIy8s7YZ+ffvrpMV1VM2fO5KGHHgLgO9/5Du+//z5XXHHFCa+vrnM3dOhQPvroI2JiYli0aBH333+/L7HXKioq4pxzzgn4Gl9//XWGDx8e9BwGsnfvXvr1O1p8oW/fvnz++efk5ubSpUsXYmJifI/XfpEBGDNmDB9//DHjxo1r8DFNZFiiOEmn90ys+0lVqC5nUJJSVFxKjcSQlBjDuaf3JDn+5C57LSsrIysri507dzJ69Gguuugi7yG1zqseGnI1xKJFi5g3b55vuWvX+kuGXHfddURHu6u1pk6dyuzZs7n11luZN28eU6dO9e13w4ajleaPHDlCUVERiYmBz2NhYSEFBQWcd54rA3bLLbdw3XXX+Z6v3W8wS5cu5W9/+xvgPojvu+8+3+O133RvuummgGM1EydO5De/+Q179uzhmmuuYfDgwUGPtWjRIr7//e/7PiS7det2wjr79++ne/fuvuXFixfz+OOPU1paSl5eHunp6b5E4f/66jp3hYWF3HLLLWzduhURoaqq6oRjJiYm+hJbU6lt+fgTkTofr9WjRw82bbKJo62JJYpwqalyl7yiJCfEc9moNPJKq+kW3/GkkwRAXFwca9asobCwkMsvv5znnnuOWbNmkZ6efsIg6Y4dO0hISCAxMZH09HRWrVpFZmZm0P3XlXD8Hzv+Wu3OnTv7fp84cSLbtm3j0KFDvPvuuzz44IMAeDweli5dSlxcXINfcyD+xwxVQxLmTTfdxPjx4/nf//1fJk+ezH//938zYMCAOtcPlqhrxcXF+c5deXk5d955JytXrqRfv348/PDDx5xX/9dX17n70Y9+xKRJk3jnnXfYuXMn559//gnHDEeLom/fvnz99dEC0Xv27KF3796kpqZSUFBAdXU1MTExvsdrlZeXN9n/v2keNkbR1Dw1UFnibkcqAh07Q4c4kuM70T+1c5MkCX/JycnMmTOHJ554gqqqKm6++WY++eQTFi1aBLiWx6xZs3zfou+9915++9vfsmXLFheux8OTTz55wn4vvvhinn32Wd9ybddTz5492bhxo69rqS4iwje/+U3uuecehg0bRkpKSsD91vctNzk5ma5du/Lxxx8D8Oqrr/paF6E688wzfa2j1157jbPPPhuACRMm+Lpo/FtP/nbs2MGAAQOYNWsWV155JWvXriUxMZGiosDjSBdffDEvvPAC1dXVAAG7noYNG8a2bduAo8k2NTWV4uJi3nrrrTpfR13nrrCwkD59+gDuaq9AalsUgX4akyQAxo4dy9atW8nOzqayspJ58+Zx5ZVXIiJMmjTJ91r+8pe/cNVVV/m227Jli29Mx7QOliiairebicpilyxiYqFjAkSFv9F2xhlnkJmZybx584iLi+O9997j0UcfZciQIYwcOZKxY8f6LhXNyMjg6aef5sYbb2TYsGGMGDGC/fv3n7DPBx98kPz8fEaMGEFmZiaLF7t7TD322GNcfvnlXHDBBfTq1StoXFOnTuV//ud/juk+mTNnDitXriQjI4Phw4fXOTbg7y9/+Qv33nsvGRkZrFmzxtefH6o5c+bwyiuvkJGRwauvvuob+H766ad58sknGTduHPv37yc5OfmEbefPn8+IESPIyspi06ZNTJs2jZSUFM466yxGjBjBvffee8z6M2bM4NRTTyUjI4PMzExef/31E/Z52WWXsWTJEgC6dOnCbbfdxsiRI7n66qsZO3Zs0NcR6Nzdd999/OIXv+Css86ipqamQeemLu+88w59+/Zl6dKlXHbZZUyePBlwlwJfeumlgLsQ4tlnn2Xy5MkMGzaM66+/nvT0dAB+97vf8eSTTzJo0CByc3N9Y2jgxmi+8Y1vNEmcpnlYradG2LhxI8OGDTv6QE01VJe5Ok1RMRATZ7cjbQVKS0uJi4tDRJg3bx5vvPEG7733XrMc++yzz+b999+nS5cuzXK8lmL16tU8+eSTvPrqq5EOpU074TOKk6v1ZGMUJ0M9UFUOntoCfvFWm6kVWbVqFTNnzkRV6dKlCy+//HKzHfsPf/gDu3fvbneJ4vDhwzzyyCORDsM0kCWKxqqudF1NKER3atCsatMynHPOOXz55ZcROfb48cHuCtx21V6dZ1oXSxQNVVHk6jNVl1kBP2NMu2CJIlSeGnenubwdoD3dOER0B2tFGGPaPEsUoSg+BDnr3byIpD6QHwNNNLPaGGNaOksUwVSVQ84GKD7o5kP0Gw/x3aBgY/3bGmNMG2HXcAaiCvk7YefHUHIIUk+H0852SaKFsDLjjbNkyRIuv/xyoO5S5HPnziUqKoq1a9f6HhsxYsQxNZ2awpo1a1i4cKFvecGCBceU6m6suXPn0r17d7Kyshg+fDgvvfTSSe1v+vTpvslzM2bMOKaMyPGWLFnCZ5995lt+4YUX+Otf/3pSxw9GVbngggs4cuRI2I5xslatWsXIkSMZNGgQs2bNCljiZMmSJSQnJ5OVlUVWVhazZ8/2PVdXKfcbbriBrVu3NstriHjZ8Ib+hL3MeGm+avYnqpsWqu5erlpRfMIqjSkzrqV5qoe3uX+bgJUZb9wxFy9erJdddpmq1l2K/JVXXtF+/frp9ddf73ssPT1ds7OzGxRTfcJVCt1/vwcPHtTU1FQ9cODAMes05P/olltu0TfffDOkdX/1q1/p73//+9CDPUnvv/++/vjHP27QNtXV1WGKJrCxY8fqZ599ph6PR6dMmaILFy48YR3/96W/YKXclyxZojNmzAh4TCszHi41VXBwA+xe6m4m1CsL+o11XU7B5GyE3Z8H/9m6CD5+Cla+4v7duij4+jkN69qyMuMnlhk/2dLbl19+OevXrw+43b/+9S8mTpzIqFGjuO666yguLgZg4cKFDB06lLPPPptZs2b5Wi6BYqmsrOShhx5i/vz5ZGVlMX/+fF8Lp7CwkLS0NDweD+AmBvbr14+qqiq2b9/OlClTGD16NOecc069xfV69OjBwIED2bVrFw8//DC33347F198MdOmTaOmpoZ7772XsWPHkpGRwZ/+9CfAfXmcOXMmw4cP57LLLiMnJ8e3v/PPP993I6N//OMfjBo1iszMTC688EJ27tzJCy+8wFNPPeX7v3v44Yd54oknAIL+P/7sZz9j3LhxnH766b5yLYFKvB/vtddeO6Y8yNVXX83o0aNJT0/nxRdf9D2ekJDAQw89xPjx41m6dCmrVq3ivPPOY/To0UyePNlXneCll15i7NixZGZmcu2111JaWhr0/NZn//79HDlyhIkTJyIiTJs2zVeIMhR1lXIHd3n3okWLfOViwskSBcCR/ZD9ERTsgi6nQdo5kBS8PEWDlB9xk/M6p7p/y5uumVxbZvzKK68EQiszfvzzgfiXGV+7di0XXHBBvdvUlhl/6qmnuOqqq3y1oPzLjN91113cfffdrFixgrfffpsZM2acsJ8FCxYwcOBA1qxZwznnnMO0adP43e9+x9q1axk5ciS//vWvfevWluE+vnR6bent1atXM3v2bO6///564/cXFRXFfffdx29/+9tjHj98+DCPPvooixYt4osvvmDMmDE8+eSTlJeXc8cdd/DBBx/wySefcOjQoaCxdOzYkdmzZzN16lTWrFlzTJmT5ORkMjMzfeXF//73vzN58mQ6dOjA7bffzjPPPMOqVat44oknAiZ5fzt27GDHjh0MGjQIcN0g7733Hq+//jp//vOfSU5OZsWKFaxYsYKXXnqJ7Oxs3nnnHTZv3sxXX33FSy+9dExXUq1Dhw5x22238fbbb/Pll1/y5ptvkpaWxve//33uvvtu3/+dv2D/j9XV1Sxfvpynn37a93htifc1a9awcuVK+vbte0Icn3766THv55dffplVq1axcuVK5syZQ25uLgAlJSWMGDGCzz//nPHjx/OjH/2It956i1WrVvHd736XBx54AIBrrrmGFStW8OWXXzJs2DD+/Oc/n3DMxYsX+7qI/H/OPPPME9bdu3fvMXEfX3Ld39KlS8nMzOSSSy5h/fr1vu2PL+Veu31UVBSDBg1qlrlA7Xswu7LEtSJKD0OnJOgzGuIaOFO2x7D61ynLh4pClyRik2HQJIirv2x30F1amXHfcerarr7S2/W56aab+M1vfkN2drbvsWXLlrFhwwbOOussACorK5k4cSKbNm1iwIAB9O/fH3BjLLXfaBsTy9SpU5k/fz6TJk1i3rx53HnnnRQXF/PZZ58d8/orKioCbj9//nw++eQTOnXqxJ/+9CdfufMrr7zSV7n1X//6F2vXrvWNPxQWFrJ161Y++ugjbrzxRqKjo+ndu3fALwnLli3j3HPP9b3eQOXU/dX3/3jNNdcAMHr0aN9YUCgl3vPy8o5578yZM8f3BeXrr79m69atpKSkEB0dzbXXXgvA5s2bWbdune9vpqamxle3bN26dTz44IMUFBRQXFzsq3Hlb9KkSSGXbNd6Sq7XGjVqFLt27SIhIYGFCxdy9dVXs3Xr1pBKtu/bty+kL38no30mCo8H8rMhdxtIlPuw73Ja+OZExHWF9KuhNM8NiJ9kkgArMx7omP5++ctf1lt6uz4xMTH85Cc/4Xe/+53vMVXloosu4o033jhm3dWrV9e5n8bEcuWVV/KLX/yCvLw8Vq1axQUXXEBJSQldunQJ6UNq6tSpx1SareV/vlSVZ5555oQPw4ULF9b7pSLYF5LG6NSpE+Au0qjtSglU4v34pBUTE4PH4yEqKoolS5awaNEili5dSnx8POeff77vPRobG+v7EqOqpKens3Tp0hPimD59Ou+++y6ZmZnMnTvXV7zR3+LFi7n77rtPeDw+Pv6E1lffvn3Zs2ePb/n4kuu1kpKSfL9feuml3HnnnRw+fLjOUu61mqtke6vpehKRbh269h5cUVXDug2bG/2zZd0X5K75Xzi8BRJ6uG6mrmnhnzgX1xVSBjZJkvBnZcYDC6X0diimT5/OokWLfF1JEyZM4NNPP/WVCS8tLWXLli0MHTqUHTt2+L4Nz58/v95YgpUrT0hIYNy4cdx1111cfvnlREdHk5SURP/+/XnzzTcB94F3Mt0OkydP5o9//KOvhbNlyxZKSko499xzmTdvHjU1Nezfv99XOdjfxIkT+fDDD32trdpy6nW9psb8PwYq8X68IUOGsGPHDsCd565duxIfH8+mTZtYtmxZwP0OGTKEQ4cO+RJFVVWVr6unqKiIXr16UVVVxWuvvRZw+9oWxfE/gbroevXqRWJiIsuWLUNV+etf/3rMmEqtAwcO+FoPy5cvx+PxkJKSUmcp91pbtmzxVewNp1aRKKLjEnslZF0yoeuFt50alXwKR4hr+E9NBwpz93Fg12ZWbD/E3qi+0PsM6BAb6Zd30qzM+ImaqvR2x44dmTVrlm9At3v37sydO9d3+e6ECRPYtGkTcXFxPP/880yZMoWzzz6bnj17+sqW1xXLpEmT2LBhg28w+3iBzt9rr73Gn//8ZzIzM0lPTz+parczZsxg+PDhjBo1ihEjRnDHHXdQXV3NN7/5TQYPHszIkSP5wQ9+EPADvXv37rz44otcc801ZGZm+mK84ooreOedd3yD2f4a+v8YqMT78fxLtk+ZMoXq6moyMjL45S9/yYQJEwLut2PHjrz11lv87Gc/IzMzk6ysLN+H/COPPML48eO56KKLfBd8nKw//vGPzJgxg0GDBjFw4EAuueQSwI3B1L7/33rrLd/f2qxZs5g3bx4iErSU+8GDB4mLi6v377AptPgy4yLSufOIC8/pev70/OjOXatjP3lu5Zvz3qh/Qx91E+byd7kxgqTeVMX35PDeXZx5xvBjruIJVaASvsYUFxeTkJCAqvLDH/6QwYMHB+yiME1n//79TJs2jX//+9+RDqXZPfXUUyQlJR1zr49aTV1mvDW0KOI6dk/T6M5dG34NWFUJHFjnajR16Ay9MqDLaXToGEtUbDxlZWVhCNe0Vy+99BJZWVmkp6dTWFjIHXfcEemQ2rxevXpx2223tegJd+HSpUsXbrnllmY5VmsYzI6SDp0a1uzRGij8Ggr3QXQ0pAyChJ7H79Z3nboxTeHuu++2FkQEXH/99ZEOISJuvfXWZjtWa0gUJziwbw8P/Pj7HD6UQ1RUFNfedAvf/t4P3JNleZC7A60u53fPv8rHn31ObFw8jzz5PMNHZjVZDE191YcxxjSFcAwntMpEER0dw09++SjDR2ZRUlzEDZeez8Qzz2Jgagd3CWqHeD7ZdIhd+3N5/+PVrF29kkfv/wmv//0/TXL82NhYcnNzSUlJsWRhjGkxVJXc3FxiY5v2Ip1WmSi69zyF7j1PAaBzQgL9004lZ8MnDBw7ys2HSOrN4jl/4Yprb0BEyBw1lqIjhRw6eMC33cmovTbaf/atMca0BLGxsQFnsZ+MVpkofCqK2LvuEzatX8fIh++H3iMhxmXSnAP7OaV3H9+qPXv1JufA/iZJFB06dPDNSDXGmLYurFc9icgUEdksIttE5OcBnu8kIvO9z38uImkh7dhTDXnbKd3xOff87CHu++XDJAwY60sSEPrUeWOMMcGFLVGISDTwHHAJMBy4UUSGH7fa94B8VR0EPAX8jnp4qkop2/oRVfl7uefhP3DZdd/hG1fffMJ6PXv15sC+o8W3Du7f1yStCWOMaW/C2aIYB2xT1R2qWgnMA46fu34V8Bfv728BF0o9X/ujyovZt2UlD/z+JfoPy2TaHbMCrnf+RZfw97fnuTIHX6wgMTHJEoUxxjRC2GZmi8i3gCmqOsO7/B1gvKrO9FtnnXedPd7l7d51Dvut0yO2/+j7tarsYoAYrT69k1SX5e3ZkSAxHX2lM6OTeuRoTVUHgJjElHxVpTp/3ymeipIEkShPTNfe+6I6xfuq2GlVRXTlwe1lQPiLuYdPKnC43rXaBzsXR9m5OMrOxVFDVDVwmeZ6hHMwO1DL4PisFMo6xA0a+0HSqMtfBTjw2n2vpt785Hca9Wr9FK3+IDXvX8+tUdWDJ7mriBGRlY2dkt/W2Lk4ys7FUXYujhKRlY3dNpxdT3uAfn7LfYF9da0jIjFAMpB33DpVnrLAFTZPhqe8CKDhNykwxph2JpyJYgUwWET6i0hH4AZgwXHrLABqi5V8C/g/PbEvrLh855qSsu0rk5sqsLLsL5LKdq4uBYqbap/GGNNWha3rSVWrRWQm8E8gGnhZVdeLyGzcTb4XAH8GXhWRbbiWxA0B9lMlIisLl3pGl+38IhXkXwUfv5ba2Lg8lSVUHtheUrFn/SrvIHtr9mL9q7Qbdi6OsnNxlJ2Loxp9Llp8mfFa3sttYzn55FYDlKlq429SYIwx7UirSRTGGGMiozXcj8IYY0wEtdhEEbbyH61QCOfiHhHZICJrReQ/InJaJOJsDvWdC7/1viUiKiJt9tLIUM6FiFzvfW+sF5HXmzvG5hLC38ipIrJYRFZ7/04ujUSc4SYiL4tIjneOWqDnRUTmeM/TWhEZFdKOVbXF/eAGv7cDA4COwJfA8OPWuRN4wfv7DcD8SMcdwXMxCYj3/v6D9nwuvOslAh8By4AxkY47gu+LwcBqoKt3uUek447guXgR+IH39+HAzkjHHaZzcS4wClhXx/OXAh/g5rBNAD4PZb8ttUURlvIfrVS950JVF6tqqXdxGW7OSlsUyvsC4BHgcaA8wHNtRSjn4jbgOVXNB1DVnGaOsbmEci4USPL+nsyJc7raBFX9iBPnovm7CvirOsuALiLSq779ttRE0Qf42m95j/exgOuoajVQCKQ0S3TNK5Rz4e97uG8MbVG950JEzgD6qer7zRlYBITyvjgdOF1EPhWRZSIypdmia16hnIuHgW+LyB5gIfCj5gmtxWno5wnQcu9H0WTlP9qAkF+niHwbGAOcF9aIIifouRCRKFwV4unNFVAEhfK+iMF1P52Pa2V+LCIjVLUgzLE1t1DOxY3AXFX9g4hMxM3fGqGqnvCH16I06nOzpbYomqr8R1sQyrlARL4BPABcqaoVxz/fRtR3LhKBEcASEdmJ64Nd0EYHtEP9G3lPVatUNRvYjEscbU0o5+J7wP8DUNWluDlZjZ6424qF9HlyvJaaKJqq/EdbUO+58Ha3/AmXJNpqPzTUcy5UtVBVU1U1TVXTcOM1V6pqo4uhtWCh/I28i7vQARFJxXVF7WjWKJtHKOdiN3AhgIgMwyWK9ngv4wXANO/VTxOAQlXdX99GLbLrSZuo/EdbEOK5+D2QALzpHc/frapXRizoMAnxXLQLIZ6LfwIXi8gGXEWCe1U1N3JRh0eI5+InwEsicjeuq2V6W/xiKSJv4LoaU73jMb8COgCo6gu48ZlLgW1AKXBrSPttg+fKGGNME2qpXU/GGGNaCEsUxhhjgrJEYYwxJihLFMYYY4KyRGGMMSYoSxSmxRGRGhFZ4/eTFmTdtLoqZTbwmEu81Ue/9Ja8GNKIfXxfRKZ5f58uIr39nvtvERnexHGuEJGsELb5sYjEn+yxTftlicK0RGWqmuX3s7OZjnuzqmbiik3+vqEbq+oLqvpX7+J0oLffczNUdUOTRHk0zucJLc4fA5YoTKNZojCtgrfl8LGIfOH9OTPAOukistzbClkrIoO9j3/b7/E/ibutbjAfAYO8217ovYfBV95a/528jz8mR+8B8oT3sYdF5Kci8i1cza3XvMeM87YExojID0Tkcb+Yp4vIM42Mcyl+Bd1E5I8islLcvSd+7X1sFi5hLRaRxd7HLhaRpd7z+KaIJNRzHNPOWaIwLVGcX7fTO97HcoCLVHUUMBWYE2C77wP/papZuA/qPd5yDVOBs7yP1wA313P8K4CvRCQWmAtMVdWRuEoGPxCRbsA3gXRVzQAe9d9YVd8CVuK++Wepapnf028B1/gtTwXmNzLOKbgyHbUeUNUxQAZwnohkqOocXC2fSao6yVvK40HgG95zuRK4p57jmHauRZbwMO1emffD0l8H4Flvn3wNrm7R8ZYCD4hIX+BvqrpVRC4ERgMrvOVN4nBJJ5DXRKQM2IkrQz0EyFbVLd7n/wL8EHgWd6+L/xaR/wVCLmmuqodEZIe3zs5W7zE+9e63IXF2xpWr8L9D2fUicjvu77oX7gY9a4/bdoL38U+9x+mIO2/G1MkShWkt7gYOApm4lvAJNyVS1ddF5HPgMuCfIjIDV1b5L6r6ixCOcbN/AUERCXh/E29toXG4InM3ADOBCxrwWuYD1wObgHdUVcV9aoccJ+4ubo8BzwHXiEh/4KfAWFXNF5G5uMJ3xxPg36p6YwPiNe2cdT2Z1iIZ2O+9f8B3cN+mjyEiA4Ad3u6WBbgumP8A3xKRHt51ukno9xTfBKSJyCDv8neAD719+smquhA3UBzoyqMiXNnzQP4GXI27R8J872MNilNVq3BdSBO83VZJQAlQKCI9gUvqiGUZcFbtaxKReBEJ1DozxscShWktngduEZFluG6nkgDrTAXWicgaYCjulo8bcB+o/xKRtcC/cd0y9VLVclx1zTdF5CvAA7yA+9B937u/D3GtnePNBV6oHcw+br/5wAbgNFVd7n2swXF6xz7+APxUVb/E3R97PfAyrjur1ovAByKyWFUP4a7IesN7nGW4c2VMnax6rDHGmKCsRWGMMSYoSxTGGGOCskRhjDEmKEsUxhhjgrJEYYwxJihLFMYYY4KyRGGMMSao/x++FN8y+TjP0QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from Lab, making ROC curves for this model, added ROC curve for our all negative model\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "def make_roc(name, clf, ytest, xtest, ax=None, labe=5, proba=True, skip=0):\n",
    "    initial=False\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "        initial=True\n",
    "    if proba:#for stuff like logistic regression\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n",
    "    else:#for stuff like SVM\n",
    "        fpr, tpr, thresholds=roc_curve(ytest, clf.decision_function(xtest))\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    if skip:\n",
    "        l=fpr.shape[0]\n",
    "        ax.plot(fpr[0:l:skip], tpr[0:l:skip], '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    else:\n",
    "        ax.plot(fpr, tpr, '.-', alpha=0.3, label='ROC curve for %s (area = %0.2f)' % (name, roc_auc))\n",
    "    label_kwargs = {}\n",
    "    label_kwargs['bbox'] = dict(\n",
    "        boxstyle='round,pad=0.3', alpha=0.2,\n",
    "    )\n",
    "    if labe!=None:    \n",
    "        for k in range(0, fpr.shape[0],labe):\n",
    "            #from https://gist.github.com/podshumok/c1d1c9394335d86255b8\n",
    "            threshold = str(np.round(thresholds[k], 2))\n",
    "            ax.annotate(threshold, (fpr[k], tpr[k]), **label_kwargs)\n",
    "    if initial:\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC')\n",
    "    fpr_0, tpr_0, thresholds_1 = metrics.roc_curve(y_test, t_repredict(clf, 01.00, X_test))\n",
    "    roc_auc_0 = auc(fpr_0, tpr_0)\n",
    "    plt.plot(fpr_0, tpr_0, '.-', alpha=0.3, label='ROC curve for all Negative Predictions (area = %0.2f)' % (roc_auc_0))\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    return ax\n",
    "\n",
    "ax=make_roc(\"logistic\",clf, y_test, X_test, labe=100, skip=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17088"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPR: 0.03144135657089025 TPR 1.0 Threshold 0.0005805368306548942\n",
      "FPR: 1.0 TPR 1.0 Threshold 8.233910943157768e-37\n",
      "FPR: 1.0 TPR 1.0 Threshold 8.233910943157768e-37\n",
      "FPR: 1.0 TPR 1.0 Threshold 8.233910943157768e-37\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fprs = [0,.1,.5,.9]\n",
    "fpr, tpr, thresholds=roc_curve(y_test, clf.predict_proba(X_test)[:,1])       \n",
    "for i in range(len(fpr)):        \n",
    "    if int(fpr[i] > 0):\n",
    "                print('FPR:', fpr[i], 'TPR', tpr[i], 'Threshold', thresholds[i] )\n",
    "                break\n",
    "for i in range(len(fpr)):             \n",
    "    if int(fpr[i]) >= .1:\n",
    "                print('FPR:', fpr[i], 'TPR', tpr[i], 'Threshold', thresholds[i] )\n",
    "                \n",
    "for i in range(len(fpr)):                 \n",
    "    if int(fpr[i]) >= .5:\n",
    "                print('FPR:', fpr[i], 'TPR', tpr[i], 'Threshold', thresholds[i] ) \n",
    "                \n",
    "for i in range(len(fpr)):      \n",
    "    if int(fpr[i]) >= .9:\n",
    "                print('FPR:', fpr[i], 'TPR', tpr[i], 'Threshold', thresholds[i] )  \n",
    "                \n",
    "                f, ax = plt.subplots()\n",
    "                ax.plot(fpr, tpr)\n",
    "                ax.set_xlabel('False Positive Rate')\n",
    "                ax.set_ylabel('True Positive Rate')\n",
    "                ax.set_title('ROC')\n",
    "                ax.legend(loc=\"lower right\")\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fprs: [0.         0.         0.         0.03144136 0.03155911 0.13565709\n",
      " 0.13577485 0.65667687 0.65679463 0.84979981 0.84991757 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print('fprs:',fpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tprs: [0.         0.00961538 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "print('tprs:',tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds: [1.99955010e+00 9.99550103e-01 4.40607006e-01 5.80536831e-04\n",
      " 5.80130136e-04 2.67749885e-04 2.67700938e-04 5.08769980e-05\n",
      " 5.08615921e-05 2.21872185e-05 2.21809340e-05 8.23391094e-37]\n"
     ]
    }
   ],
   "source": [
    "print('thresholds:',thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 3: Missing data\n",
    "\n",
    "In this problem you are given a different data set, `hw6_dataset_missing.csv`, that is  similar to the one you used above (same column definitions and same conditions), however this data set contains missing values. \n",
    "\n",
    "*Note*: be careful of reading/treating column names and row names in this data set as well, it *may* be different than the first data set.\n",
    "\n",
    "\n",
    "1. Remove all observations that contain and missing values, split the dataset into a 75-25 train-test split, and fit the regularized logistic regression as in Question 1 (use `LogisticRegressionCV` again to retune).  Report the overall classification rate and TPR in the test set.\n",
    "2. Restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via mean imputation.  Split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
    "3. Again restart with a fresh copy of the data in `hw6_dataset_missing.csv` and impute the missing data via a model-based imputation method. Once again split the data 75-25 and fit the regularized logistic regression model.  Report the overall classification rate and TPR in the test set.  \n",
    "4. Compare the results in the 3 previous parts of this problem.  Prepare a paragraph (5-6 sentences) discussing the results, the computational complexity of the methods, and conjecture and explain why you get the results that you see.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data set into a training set and a testing set\n",
    "np.random.seed(9001)\n",
    "df = pd.read_csv('HW6_dataset_missing.csv')\n",
    "msk = np.random.rand(len(df)) < 0.75\n",
    "data_train = df[msk]\n",
    "data_test = df[~msk]\n",
    "data_train = data_train.dropna()\n",
    "data_test = data_test.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>-0.2160</td>\n",
       "      <td>0.2880</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>-0.993</td>\n",
       "      <td>-0.9550</td>\n",
       "      <td>-1.620</td>\n",
       "      <td>-1.470</td>\n",
       "      <td>-1.0100</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.1900</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>0.395</td>\n",
       "      <td>2.060</td>\n",
       "      <td>-1.180</td>\n",
       "      <td>-2.8500</td>\n",
       "      <td>-1.290</td>\n",
       "      <td>-2.100</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>0.3130</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>-0.2790</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.4320</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0181</td>\n",
       "      <td>0.2480</td>\n",
       "      <td>-0.869</td>\n",
       "      <td>-0.190</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.6980</td>\n",
       "      <td>0.363</td>\n",
       "      <td>1.030</td>\n",
       "      <td>-0.2490</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>0.5020</td>\n",
       "      <td>-0.342</td>\n",
       "      <td>-0.2740</td>\n",
       "      <td>-0.172</td>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0702</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.440</td>\n",
       "      <td>-0.2880</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.2170</td>\n",
       "      <td>-0.3570</td>\n",
       "      <td>-0.0539</td>\n",
       "      <td>-0.0688</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.6380</td>\n",
       "      <td>0.436</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0622</td>\n",
       "      <td>0.269</td>\n",
       "      <td>-0.217</td>\n",
       "      <td>-1.030</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.472</td>\n",
       "      <td>-0.390</td>\n",
       "      <td>0.3660</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.0846</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.4240</td>\n",
       "      <td>0.3520</td>\n",
       "      <td>-0.259</td>\n",
       "      <td>-0.0947</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>0.3020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7190</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.704</td>\n",
       "      <td>0.8530</td>\n",
       "      <td>0.953</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.1190</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       1       2       3       4      5       6      7      8  \\\n",
       "0           0  0.1290 -0.2160  0.2880  0.2370 -0.993 -0.9550 -1.620 -1.470   \n",
       "1           1  0.0989  0.1160  0.3130  0.2810 -0.188 -0.2790  0.173  0.445   \n",
       "2           2  0.0215  0.1590  0.5790  0.5020 -0.342 -0.2740 -0.172 -0.164   \n",
       "3           3 -0.2170 -0.3570 -0.0539 -0.0688  0.445  0.6380  0.436  0.351   \n",
       "4           4 -0.0846  0.0166  0.4240  0.3520 -0.259 -0.0947  0.119 -0.162   \n",
       "\n",
       "        9  ...     109     110    111    112    113     114    115    116  \\\n",
       "0 -1.0100  ... -1.1900  1.1000  0.395  2.060 -1.180 -2.8500 -1.290 -2.100   \n",
       "1  0.4320  ... -0.0181  0.2480 -0.869 -0.190  0.451  0.6980  0.363  1.030   \n",
       "2  0.2160  ...  0.0702  0.0200  0.397 -0.800  0.173  0.7380  0.465  0.440   \n",
       "3  0.0401  ...     NaN  0.0622  0.269 -0.217 -1.030  0.0276  0.472 -0.390   \n",
       "4  0.3020  ...  0.7190  0.3250 -0.286 -0.528 -0.704  0.8530  0.953 -0.116   \n",
       "\n",
       "      117  type  \n",
       "0  0.0121   0.0  \n",
       "1 -0.2490   0.0  \n",
       "2 -0.2880   0.0  \n",
       "3  0.3660   0.0  \n",
       "4 -0.1190   0.0  \n",
       "\n",
       "[5 rows x 119 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = data_train['type'].values\n",
    "X_train = data_train.values\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "y_test = data_test['type'].values\n",
    "X_test = data_test.values\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\model_selection\\_split.py:657: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\linesearch.py:466: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\scipy\\optimize\\linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The optimized L2 regularization paramater id: [1.e-10]\n",
      "Estimated beta1: \n",
      " [[-1.17595780e-05 -1.78250598e-11 -2.31045512e-11  1.41727317e-10\n",
      "   1.18666748e-10 -2.39446150e-10 -2.27394977e-10 -6.85839585e-11\n",
      "  -8.40709297e-11  8.00349653e-10  5.77407755e-10 -1.72655274e-11\n",
      "  -8.44046421e-12 -2.06844162e-10  6.54734945e-11 -6.54233929e-11\n",
      "  -6.05646962e-11 -2.35675209e-10 -5.53537601e-11  5.87062023e-11\n",
      "   1.83248946e-10  1.90464943e-10  6.92748751e-13  1.06316162e-10\n",
      "   1.56077040e-10  1.85828079e-10  1.57423826e-10  2.28985016e-10\n",
      "  -1.40231995e-10  2.34265482e-10 -1.28060098e-10  2.07636174e-11\n",
      "  -1.88732612e-11  2.92662159e-11  3.32234210e-11  2.46288043e-10\n",
      "   4.33199414e-11 -6.38918520e-11 -5.61963191e-11 -2.60100585e-11\n",
      "  -1.18539164e-10  1.28102675e-10  1.42220782e-10  1.53985825e-10\n",
      "   5.36602956e-11  1.81544272e-10  2.00307342e-10  1.35814353e-10\n",
      "  -2.70293198e-10 -1.51136843e-10 -1.37482594e-10 -1.39855466e-10\n",
      "  -2.97760620e-10 -8.26035727e-11 -3.18119898e-11 -3.57025154e-11\n",
      "  -3.62735586e-11 -3.69107380e-11  3.58233211e-10  3.50806770e-11\n",
      "   3.00840056e-10 -1.36307220e-10  3.06786176e-10  3.20689410e-10\n",
      "   3.30777418e-10  3.37629531e-10 -3.38086914e-10 -2.15082877e-10\n",
      "  -5.15665387e-10 -2.15561966e-10 -2.19635557e-10 -1.35609573e-10\n",
      "  -2.24597842e-10 -7.94492004e-11  6.53351433e-11  6.36038956e-11\n",
      "   6.14738070e-11  1.37629200e-10 -1.79914562e-10  6.92961876e-11\n",
      "  -1.68347410e-10 -1.66614268e-10  4.86093011e-11  9.02540299e-11\n",
      "   2.88458346e-11  2.92014773e-11 -1.25082667e-10 -1.22497125e-10\n",
      "  -1.19348923e-10 -1.87535560e-11 -2.20503526e-10 -2.19925532e-10\n",
      "  -2.19514651e-10 -5.52150875e-11 -6.57167671e-11 -1.53295703e-10\n",
      "  -6.93177335e-11 -1.47775449e-11 -5.54394861e-11  4.61906070e-11\n",
      "  -1.12625301e-12 -1.15670205e-10 -1.06026254e-10 -1.06265874e-10\n",
      "   3.31354538e-11 -3.99830184e-10 -2.25205836e-10 -6.10352910e-11\n",
      "  -2.33582743e-11  1.02991219e-10  9.80061220e-11 -1.61817506e-10\n",
      "   4.52934793e-11 -1.83495534e-10  8.66076272e-11  2.07329000e-10\n",
      "   1.07786394e-12  2.98556619e-11  1.97969510e-10]]\n",
      "Estimated beta0: \n",
      " [-4.43327339]\n",
      "\n",
      "\n",
      "Test Set Confusion matrix:\n",
      "[[325   0]\n",
      " [  1   0]]\n",
      "The training classification accuracy is:  0.9981981981981982\n",
      "The testing classification accuracy is:  0.9969325153374233\n",
      "The precision score on the test set is:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Fit a logistic regression classifier to the training set and report the accuracy of the classifier on the test set\n",
    "clf = LogisticRegressionCV(\n",
    "        Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
    "        ,penalty='l2'\n",
    "        ,cv=10\n",
    "        ,random_state=777\n",
    "        ,fit_intercept=True\n",
    "        ,solver='newton-cg'\n",
    "        ,tol=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# L2 Regularization parameter\n",
    "print('\\n')\n",
    "print(\"The optimized L2 regularization paramater id:\", clf.C_)\n",
    "\n",
    "# The coefficients\n",
    "print('Estimated beta1: \\n', clf.coef_)\n",
    "print('Estimated beta0: \\n', clf.intercept_)\n",
    "\n",
    "# Metrics\n",
    "print('\\n')\n",
    "print('Test Set Confusion matrix:') \n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "y_prediction = clf.predict(X_test)\n",
    "test_precision = precision_score(y_test, y_prediction)\n",
    "print('The training classification accuracy is: ', train_score)\n",
    "print('The testing classification accuracy is: ', test_score)\n",
    "print('The precision score on the test set is: ', test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data set into a training set and a testing set\n",
    "np.random.seed(9001)\n",
    "df_2 = pd.read_csv('HW6_dataset_missing.csv')\n",
    "msk = np.random.rand(len(df)) < 0.75\n",
    "data_train_2 = df_2[msk]\n",
    "data_test_2 = df_2[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The optimized L2 regularization paramater id: [10.]\n",
      "Estimated beta1: \n",
      " [[-4.19567252e-05 -1.09860282e-01  1.94387677e-01  3.15148437e-01\n",
      "   2.73434229e-01 -2.46971540e-01 -2.54965939e-01  1.02013192e-01\n",
      "   6.14227494e-02  1.05245748e-01 -7.58985603e-02  5.90299628e-02\n",
      "   3.77271647e-02 -1.54869244e-01 -7.69420198e-03 -1.55536537e-01\n",
      "  -2.46316467e-01  3.94972781e-01 -7.45866374e-02 -5.32129649e-02\n",
      "   1.49194117e-01  6.01930936e-02 -1.17254181e-01  1.93678317e-01\n",
      "   5.06511394e-01  2.61355381e-01  1.09219280e-01  4.23571589e-02\n",
      "  -1.24707430e-01 -7.79146184e-02  1.50168672e-01 -1.19177327e-01\n",
      "  -1.33582063e-01  2.03218055e-01  1.62411745e-01  5.01622719e-02\n",
      "   1.36557953e-01 -1.84801829e-01 -1.38748718e-01  1.40862788e-01\n",
      "   1.66858096e-01 -1.42890884e-01 -1.06513470e-01 -1.19651211e-01\n",
      "  -1.10828295e-01  1.57099883e-01  1.31374967e-01  1.38635392e-01\n",
      "  -2.56173080e-01 -7.32143632e-02  2.20742490e-02 -8.89628637e-02\n",
      "   5.08155621e-02 -2.76114169e-03 -2.88322717e-02 -2.79304197e-02\n",
      "  -3.00944761e-02 -3.33482266e-02  3.29227163e-01 -1.27895251e-01\n",
      "   5.22313714e-02  5.13556963e-02  5.54207528e-02  8.54766414e-02\n",
      "   1.07469388e-01  1.21384474e-01  2.37488625e-01  1.00619850e-01\n",
      "   1.07311083e-01 -2.10902046e-02 -1.72779921e-02 -7.90853791e-03\n",
      "  -2.36491177e-02 -5.52939020e-02  1.08246858e-01  5.61563726e-02\n",
      "   7.36947297e-03 -1.89263056e-01  2.28708009e-01 -8.46889597e-02\n",
      "   1.13814426e-01  9.32623830e-02 -1.91776841e-01  2.89767894e-01\n",
      "   1.03450460e-02 -1.93870941e-02 -1.45489540e-01 -6.56026646e-02\n",
      "   4.10026782e-02  1.38600455e-01 -5.52496105e-03 -1.36831718e-02\n",
      "  -2.29373880e-02 -1.95799611e-01 -3.50678846e-02  8.67973919e-02\n",
      "   1.38811967e-01  2.73345437e-02 -7.79460405e-05 -7.00511203e-02\n",
      "   1.13694761e-01  2.81385509e-01 -6.75396353e-02 -1.79751235e-02\n",
      "  -1.91040289e-01 -2.59400734e-01 -1.03803673e-01 -8.80618968e-02\n",
      "   9.23714987e-02  9.30651796e-02 -8.19479540e-02 -2.31487697e-01\n",
      "  -2.29342665e-01 -3.37949791e-01  1.13745834e-01  2.89999389e-01\n",
      "  -7.52109077e-02  5.06654693e-02  9.82385478e+00]]\n",
      "Estimated beta0: \n",
      " [-8.25507911]\n",
      "\n",
      "\n",
      "Test Set Confusion matrix:\n",
      "[[6074    0]\n",
      " [   3   48]]\n",
      "The training classification accuracy is:  1.0\n",
      "The testing classification accuracy is:  0.9995102040816326\n",
      "The precision score on the test set is:  1.0\n"
     ]
    }
   ],
   "source": [
    "for column in data_train_2:\n",
    "    data_train_2[column] = data_train_2[column].fillna(data_train_2[column].mean())\n",
    "for column in data_test_2:\n",
    "    data_test_2[column] = data_test_2[column].fillna(data_train_2[column].mean())\n",
    "    \n",
    "y_train = data_train_2['type'].values\n",
    "X_train = data_train_2.values\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "y_test = data_test_2['type'].values\n",
    "X_test = data_test_2.values\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "\n",
    "# Fit a logistic regression classifier to the training set and report the accuracy of the classifier on the test set\n",
    "clf = LogisticRegressionCV(\n",
    "        Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
    "        ,penalty='l2'\n",
    "        ,cv=10\n",
    "        ,random_state=777\n",
    "        ,fit_intercept=True\n",
    "        ,solver='newton-cg'\n",
    "        ,tol=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# L2 Regularization parameter\n",
    "print('\\n')\n",
    "print(\"The optimized L2 regularization paramater id:\", clf.C_)\n",
    "\n",
    "# The coefficients\n",
    "print('Estimated beta1: \\n', clf.coef_)\n",
    "print('Estimated beta0: \\n', clf.intercept_)\n",
    "\n",
    "# Metrics\n",
    "print('\\n')\n",
    "print('Test Set Confusion matrix:') \n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "\n",
    "train_score = clf.score(X_train, y_train)\n",
    "test_score = clf.score(X_test, y_test)\n",
    "y_prediction = clf.predict(X_test)\n",
    "test_precision = precision_score(y_test, y_prediction)\n",
    "print('The training classification accuracy is: ', train_score)\n",
    "print('The testing classification accuracy is: ', test_score)\n",
    "print('The precision score on the test set is: ', test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        0.027000\n",
       "2       -0.527000\n",
       "4       -1.550000\n",
       "7        1.270000\n",
       "8        0.571000\n",
       "11       0.128000\n",
       "17       0.424000\n",
       "19      -1.110000\n",
       "20      -0.426000\n",
       "22      -0.681000\n",
       "26      -0.430000\n",
       "29       0.364000\n",
       "30       0.399000\n",
       "31      -0.926000\n",
       "36       0.541000\n",
       "37      -0.924000\n",
       "38       0.650000\n",
       "43       0.708000\n",
       "50       2.390000\n",
       "54       0.825000\n",
       "65       0.430000\n",
       "70      -0.489000\n",
       "71       0.134000\n",
       "74      -0.086000\n",
       "75      -0.553000\n",
       "78      -0.427000\n",
       "79       0.526000\n",
       "87      -0.458000\n",
       "91       0.305000\n",
       "102     -1.230000\n",
       "104     -0.863000\n",
       "105     -1.350000\n",
       "114     -0.500000\n",
       "115      2.490000\n",
       "116     -0.199000\n",
       "119     -2.050000\n",
       "121      0.978000\n",
       "122     -1.390000\n",
       "132     -1.040000\n",
       "135      0.780000\n",
       "136     -0.038100\n",
       "138      1.780000\n",
       "139     -0.586000\n",
       "141     -0.499000\n",
       "143     -0.461000\n",
       "144      0.281000\n",
       "155     -1.850000\n",
       "158     -0.734000\n",
       "160      0.192000\n",
       "164      0.234000\n",
       "168     -0.172000\n",
       "173     -0.782000\n",
       "174     -1.020000\n",
       "175     -0.166000\n",
       "177     -1.020000\n",
       "184     -0.203000\n",
       "202     -0.489000\n",
       "205     -0.614000\n",
       "206     -0.442000\n",
       "208      0.778000\n",
       "209     -0.246000\n",
       "210      0.677000\n",
       "211     -1.470000\n",
       "219     -1.140000\n",
       "220     -0.561000\n",
       "225     -0.622000\n",
       "227      0.575000\n",
       "230      3.130000\n",
       "231     -0.185000\n",
       "238     -0.369000\n",
       "243     -0.847000\n",
       "244     -0.460000\n",
       "255     -0.703000\n",
       "259     -0.620000\n",
       "261     -0.770000\n",
       "267     -0.421000\n",
       "271      0.248000\n",
       "274      1.740000\n",
       "275      1.850000\n",
       "277     -2.340000\n",
       "279     -0.168000\n",
       "282      0.781000\n",
       "283     -0.557000\n",
       "288     -0.274000\n",
       "300      0.058200\n",
       "305      0.406000\n",
       "306     -0.158000\n",
       "313     -0.535000\n",
       "321     -0.278000\n",
       "326     -0.928000\n",
       "327     -0.633000\n",
       "329     -0.424000\n",
       "330      0.040100\n",
       "333     -1.050000\n",
       "335      1.960000\n",
       "336      0.591000\n",
       "337      0.981000\n",
       "339     -1.800000\n",
       "341      0.567000\n",
       "345      0.048600\n",
       "350      0.401000\n",
       "358      3.190000\n",
       "363      0.252000\n",
       "369     -2.840000\n",
       "370     -0.134000\n",
       "371     -0.924000\n",
       "378     -0.758000\n",
       "380     -1.060000\n",
       "381      0.408000\n",
       "382     -0.283000\n",
       "386      0.512000\n",
       "392      0.497000\n",
       "396      3.080000\n",
       "401     -1.050000\n",
       "407      2.040000\n",
       "412     -0.774000\n",
       "416     -0.539000\n",
       "417     -0.533000\n",
       "421     -0.194000\n",
       "429      0.295000\n",
       "437     -0.904000\n",
       "445     -1.400000\n",
       "446      0.411000\n",
       "449      0.295000\n",
       "454     -1.080000\n",
       "455     -0.107000\n",
       "458     -0.448000\n",
       "460     -0.337000\n",
       "467      2.770000\n",
       "472     -1.260000\n",
       "473      0.256000\n",
       "487      2.150000\n",
       "489     -0.987000\n",
       "492     -0.749000\n",
       "493      0.251000\n",
       "494     -0.423000\n",
       "496     -1.460000\n",
       "497     -0.358000\n",
       "511     -0.688000\n",
       "514      0.453000\n",
       "518     -0.586000\n",
       "522      0.870000\n",
       "525     -0.386000\n",
       "530      0.774000\n",
       "535      0.765000\n",
       "536      1.830000\n",
       "537      0.652000\n",
       "539     -0.422000\n",
       "540     -0.192000\n",
       "548      0.067000\n",
       "551      1.780000\n",
       "552      0.465000\n",
       "553     -0.053300\n",
       "555      0.322000\n",
       "566      0.169000\n",
       "572      0.787000\n",
       "575     -0.061300\n",
       "586     -0.093100\n",
       "587      0.947000\n",
       "602     -0.513000\n",
       "603     -0.342000\n",
       "605     -0.349000\n",
       "607     -0.173000\n",
       "609     -0.257000\n",
       "610     -0.542000\n",
       "615      1.000000\n",
       "618     -0.761000\n",
       "623      1.170000\n",
       "626      1.410000\n",
       "629      0.740000\n",
       "639      0.405000\n",
       "640     -0.689000\n",
       "657      0.666000\n",
       "663      0.465000\n",
       "667     -1.080000\n",
       "668     -0.355000\n",
       "669     -0.579000\n",
       "681     -0.131000\n",
       "684      0.427000\n",
       "690      1.040000\n",
       "691      1.280000\n",
       "699      0.238000\n",
       "701      1.130000\n",
       "716     -1.290000\n",
       "717      2.260000\n",
       "730      0.809000\n",
       "731     -0.973000\n",
       "733     -0.847000\n",
       "741      0.547000\n",
       "742     -1.290000\n",
       "749     -0.484000\n",
       "754     -0.923000\n",
       "755      0.213000\n",
       "756      0.224000\n",
       "762     -2.090000\n",
       "763      1.050000\n",
       "771      0.223000\n",
       "775      0.832000\n",
       "782     -0.581000\n",
       "784     -2.130000\n",
       "788     -0.152000\n",
       "789      0.157000\n",
       "793      0.960000\n",
       "794      0.412000\n",
       "795      0.155000\n",
       "798      0.963000\n",
       "801      0.048300\n",
       "802     -0.979000\n",
       "805     -0.260000\n",
       "812      0.308000\n",
       "814     -0.110000\n",
       "819      0.899000\n",
       "826     -0.690000\n",
       "828     -0.846000\n",
       "830      0.325000\n",
       "831     -1.580000\n",
       "835      0.641000\n",
       "836      0.980000\n",
       "838      0.076500\n",
       "840      1.190000\n",
       "843      2.190000\n",
       "849     -2.250000\n",
       "850     -0.550000\n",
       "851      0.390000\n",
       "858      0.461000\n",
       "876     -1.280000\n",
       "883     -0.669000\n",
       "889     -0.413000\n",
       "891     -0.715000\n",
       "892     -0.723000\n",
       "897     -0.287000\n",
       "898      0.749000\n",
       "899      0.084800\n",
       "903      0.767000\n",
       "907      0.094500\n",
       "908     -0.164000\n",
       "909     -0.814000\n",
       "915     -0.144000\n",
       "920      1.240000\n",
       "921      0.851000\n",
       "927     -0.647000\n",
       "936     -0.223000\n",
       "939     -0.435000\n",
       "944     -0.086500\n",
       "948      0.021300\n",
       "949      0.328000\n",
       "950      0.033400\n",
       "952      0.276000\n",
       "953     -0.390000\n",
       "955      2.080000\n",
       "959      0.296000\n",
       "960     -0.292000\n",
       "963      0.492000\n",
       "965      0.637000\n",
       "973      0.012600\n",
       "975      0.556000\n",
       "978     -0.909000\n",
       "980     -1.010000\n",
       "981     -0.413000\n",
       "982      0.300000\n",
       "989     -0.746000\n",
       "993      0.146000\n",
       "994     -0.609000\n",
       "999     -0.182000\n",
       "1000     0.153000\n",
       "1005     0.265000\n",
       "1008     0.069500\n",
       "1012     0.232000\n",
       "1016    -0.406000\n",
       "1020    -1.050000\n",
       "1021    -0.266000\n",
       "1027    -0.455000\n",
       "1030    -1.680000\n",
       "1036     0.214000\n",
       "1039    -1.630000\n",
       "1042     0.102000\n",
       "1043    -0.822000\n",
       "1046     1.730000\n",
       "1054    -0.858000\n",
       "1065     1.700000\n",
       "1076     1.130000\n",
       "1078     1.370000\n",
       "1080    -0.651000\n",
       "1090     0.362000\n",
       "1091    -0.570000\n",
       "1092     0.284000\n",
       "1100    -1.210000\n",
       "1103     1.000000\n",
       "1108     1.120000\n",
       "1109    -0.529000\n",
       "1110     0.036900\n",
       "1111    -0.965000\n",
       "1112    -0.101000\n",
       "1114     1.330000\n",
       "1127    -0.541000\n",
       "1134     0.782000\n",
       "1136    -0.467000\n",
       "1140     0.152000\n",
       "1149     0.000445\n",
       "1158    -0.916000\n",
       "1163     0.930000\n",
       "1173     0.197000\n",
       "1176    -1.000000\n",
       "1181    -0.320000\n",
       "1183     0.936000\n",
       "1185    -0.865000\n",
       "1186     2.170000\n",
       "1187    -0.740000\n",
       "1190     0.776000\n",
       "1198     1.050000\n",
       "1200    -0.865000\n",
       "1209    -0.186000\n",
       "1211     1.150000\n",
       "1219    -0.579000\n",
       "1224     4.670000\n",
       "1227     0.417000\n",
       "1231     0.100000\n",
       "1238    -0.510000\n",
       "1245    -0.096100\n",
       "1247    -0.205000\n",
       "1249    -1.680000\n",
       "1250    -0.257000\n",
       "1258     0.978000\n",
       "1260     1.020000\n",
       "1261     0.232000\n",
       "1267     0.049000\n",
       "1270    -2.280000\n",
       "1271     0.421000\n",
       "1273     3.260000\n",
       "1279    -0.419000\n",
       "1281     1.470000\n",
       "1284    -1.060000\n",
       "1300    -0.013400\n",
       "1305    -0.064500\n",
       "1306     0.197000\n",
       "1307     0.091600\n",
       "1313     1.790000\n",
       "1319     0.991000\n",
       "1327    -0.298000\n",
       "1333    -0.764000\n",
       "1346    -0.004520\n",
       "1348    -2.010000\n",
       "1353     0.252000\n",
       "1362     0.635000\n",
       "1364    -0.799000\n",
       "1367     1.020000\n",
       "1368     0.212000\n",
       "1377    -0.436000\n",
       "1379    -0.625000\n",
       "1388    -0.388000\n",
       "1395    -0.609000\n",
       "1417    -0.923000\n",
       "1420    -0.149000\n",
       "1425    -0.367000\n",
       "1426     0.659000\n",
       "1428     1.830000\n",
       "1437     0.008270\n",
       "1443     0.049800\n",
       "1445    -0.926000\n",
       "1453    -0.701000\n",
       "1454    -0.599000\n",
       "1457     0.321000\n",
       "1459    -0.500000\n",
       "1460    -0.275000\n",
       "1461     0.875000\n",
       "1464     1.260000\n",
       "1469    -0.507000\n",
       "1472    -0.667000\n",
       "1473     0.044500\n",
       "1476    -0.844000\n",
       "1480    -0.201000\n",
       "1481    -0.963000\n",
       "1484     0.706000\n",
       "1485     0.104000\n",
       "1490    -0.996000\n",
       "1492    -0.413000\n",
       "1494     1.480000\n",
       "1495    -0.648000\n",
       "1509     0.795000\n",
       "1526    -0.285000\n",
       "1528    -0.206000\n",
       "1529     0.388000\n",
       "1531    -0.282000\n",
       "1533     0.994000\n",
       "1535     1.550000\n",
       "1542    -1.440000\n",
       "1543    -0.079900\n",
       "1545    -0.598000\n",
       "1549     0.694000\n",
       "1550    -0.062900\n",
       "1553    -1.720000\n",
       "1558     0.443000\n",
       "1560    -0.401000\n",
       "1563    -0.085800\n",
       "1565     0.131000\n",
       "1573    -0.982000\n",
       "1578    -0.028500\n",
       "1580     1.370000\n",
       "1584     0.381000\n",
       "1587    -0.257000\n",
       "1588    -0.240000\n",
       "1590    -0.473000\n",
       "1596    -1.340000\n",
       "1597    -1.220000\n",
       "1598     0.104000\n",
       "1600    -0.155000\n",
       "1609     0.101000\n",
       "1610    -0.136000\n",
       "1614     0.999000\n",
       "1615    -0.586000\n",
       "1616    -0.830000\n",
       "1620    -0.430000\n",
       "1623     0.682000\n",
       "1629     1.150000\n",
       "1631    -0.477000\n",
       "1636     1.910000\n",
       "1641    -0.835000\n",
       "1644     0.860000\n",
       "1649    -1.280000\n",
       "1651     0.293000\n",
       "1660     0.753000\n",
       "1664    -0.669000\n",
       "1669     0.066200\n",
       "1674     0.783000\n",
       "1675     0.745000\n",
       "1676     0.983000\n",
       "1678    -0.938000\n",
       "1679    -0.234000\n",
       "1683    -0.553000\n",
       "1684    -0.553000\n",
       "1685    -0.360000\n",
       "1686    -0.386000\n",
       "1697    -0.982000\n",
       "1699     1.070000\n",
       "1703     0.526000\n",
       "1706     0.530000\n",
       "1709    -0.731000\n",
       "1710    -0.191000\n",
       "1716     0.089400\n",
       "1717    -0.254000\n",
       "1725     0.579000\n",
       "1730    -0.860000\n",
       "1732     0.507000\n",
       "1735    -0.395000\n",
       "1737    -0.718000\n",
       "1740     0.739000\n",
       "1747     1.100000\n",
       "1751     0.291000\n",
       "1752     1.920000\n",
       "1754     1.280000\n",
       "1756     0.178000\n",
       "1769     0.202000\n",
       "1780     0.029000\n",
       "1783    -0.586000\n",
       "1799    -1.180000\n",
       "1800     0.291000\n",
       "1806    -0.973000\n",
       "1807    -0.691000\n",
       "1808     1.050000\n",
       "1813    -0.700000\n",
       "1815    -0.128000\n",
       "1818     0.225000\n",
       "1822     0.709000\n",
       "1823    -0.313000\n",
       "1829     0.997000\n",
       "1837    -0.650000\n",
       "1842     0.357000\n",
       "1850    -0.403000\n",
       "1852    -0.481000\n",
       "1853    -0.589000\n",
       "1856    -0.957000\n",
       "1862    -1.220000\n",
       "1874    -0.241000\n",
       "1890    -1.290000\n",
       "1892    -0.761000\n",
       "1893     0.205000\n",
       "1898    -0.654000\n",
       "1899     0.816000\n",
       "1901     0.032600\n",
       "1908     1.670000\n",
       "1910    -0.971000\n",
       "1918     0.362000\n",
       "1920     0.496000\n",
       "1922     2.350000\n",
       "1927     0.058300\n",
       "1936     0.713000\n",
       "1946    -0.698000\n",
       "1947    -0.060500\n",
       "1948    -0.341000\n",
       "1950    -1.260000\n",
       "1956    -0.117000\n",
       "1961    -1.080000\n",
       "1967    -1.290000\n",
       "1972     0.565000\n",
       "1975    -0.482000\n",
       "1983    -0.932000\n",
       "1986    -0.131000\n",
       "1991    -0.232000\n",
       "1992     0.451000\n",
       "1994    -0.047200\n",
       "           ...   \n",
       "22904   -0.097900\n",
       "22905    0.639000\n",
       "22907    0.340000\n",
       "22919   -0.099100\n",
       "22921    0.332000\n",
       "22927    0.177000\n",
       "22928    1.180000\n",
       "22935    0.130000\n",
       "22939    0.440000\n",
       "22943   -0.372000\n",
       "22944    0.123000\n",
       "22946   -0.363000\n",
       "22949   -0.236000\n",
       "22958   -0.901000\n",
       "22965    0.498000\n",
       "22970   -0.403000\n",
       "22975   -1.120000\n",
       "22978   -1.220000\n",
       "22979   -0.933000\n",
       "22985   -0.711000\n",
       "22998    0.862000\n",
       "23001   -0.200000\n",
       "23013   -1.000000\n",
       "23020    1.620000\n",
       "23032    1.420000\n",
       "23037   -1.160000\n",
       "23046    0.958000\n",
       "23047    0.991000\n",
       "23049   -0.517000\n",
       "23051   -0.290000\n",
       "23052   -0.326000\n",
       "23054   -0.775000\n",
       "23067   -0.135000\n",
       "23070   -0.592000\n",
       "23071    1.960000\n",
       "23079   -0.585000\n",
       "23097    2.050000\n",
       "23102    0.513000\n",
       "23106   -0.900000\n",
       "23114    0.693000\n",
       "23117    1.000000\n",
       "23121   -1.270000\n",
       "23133   -0.777000\n",
       "23136   -1.660000\n",
       "23145   -0.439000\n",
       "23147    0.735000\n",
       "23148    0.797000\n",
       "23164    0.402000\n",
       "23166   -1.080000\n",
       "23169   -0.085100\n",
       "23171   -0.526000\n",
       "23177    0.824000\n",
       "23187    1.360000\n",
       "23193    0.203000\n",
       "23194   -1.000000\n",
       "23197   -0.470000\n",
       "23205    1.290000\n",
       "23206    0.217000\n",
       "23209   -0.796000\n",
       "23211    0.313000\n",
       "23213    0.172000\n",
       "23216   -0.946000\n",
       "23218   -0.377000\n",
       "23222   -0.323000\n",
       "23225   -0.537000\n",
       "23236   -0.453000\n",
       "23238   -0.292000\n",
       "23241   -0.715000\n",
       "23244    2.540000\n",
       "23249    0.188000\n",
       "23250   -0.928000\n",
       "23252   -1.280000\n",
       "23253   -0.308000\n",
       "23267    2.110000\n",
       "23269   -0.705000\n",
       "23271   -0.838000\n",
       "23278   -0.611000\n",
       "23281    0.882000\n",
       "23282   -0.371000\n",
       "23291   -0.930000\n",
       "23293   -0.124000\n",
       "23295   -0.664000\n",
       "23298   -0.927000\n",
       "23308   -0.589000\n",
       "23311   -0.829000\n",
       "23315   -0.268000\n",
       "23316    0.173000\n",
       "23325   -0.720000\n",
       "23341   -1.220000\n",
       "23347   -0.468000\n",
       "23357    1.270000\n",
       "23369   -0.839000\n",
       "23372   -1.020000\n",
       "23376   -0.366000\n",
       "23381    0.537000\n",
       "23384   -0.468000\n",
       "23392   -0.332000\n",
       "23394   -0.301000\n",
       "23399    0.259000\n",
       "23400    0.302000\n",
       "23410    0.593000\n",
       "23414   -0.892000\n",
       "23419   -0.454000\n",
       "23423   -0.667000\n",
       "23424   -0.854000\n",
       "23426    0.840000\n",
       "23431   -0.448000\n",
       "23432    0.726000\n",
       "23435    0.679000\n",
       "23441   -2.430000\n",
       "23442   -2.080000\n",
       "23443    0.142000\n",
       "23444   -1.830000\n",
       "23446    0.104000\n",
       "23450    0.285000\n",
       "23451   -0.429000\n",
       "23455   -0.897000\n",
       "23457    0.492000\n",
       "23458    1.770000\n",
       "23467   -0.645000\n",
       "23471   -0.559000\n",
       "23473    2.200000\n",
       "23487    1.340000\n",
       "23493   -0.583000\n",
       "23501    0.604000\n",
       "23509   -0.682000\n",
       "23515   -0.761000\n",
       "23516   -1.390000\n",
       "23521   -2.340000\n",
       "23523   -0.353000\n",
       "23537   -0.426000\n",
       "23542   -0.146000\n",
       "23545    0.822000\n",
       "23555    0.493000\n",
       "23563   -1.100000\n",
       "23573   -0.205000\n",
       "23590    0.519000\n",
       "23593    1.350000\n",
       "23597   -1.260000\n",
       "23598   -0.390000\n",
       "23602   -0.592000\n",
       "23605   -1.530000\n",
       "23606   -0.095000\n",
       "23609   -0.316000\n",
       "23616    0.552000\n",
       "23617   -0.108000\n",
       "23625    0.508000\n",
       "23627   -0.673000\n",
       "23633   -0.230000\n",
       "23645   -0.517000\n",
       "23647    0.440000\n",
       "23655    4.090000\n",
       "23656   -0.516000\n",
       "23663    0.363000\n",
       "23664   -0.301000\n",
       "23666   -0.691000\n",
       "23670   -0.595000\n",
       "23675   -0.843000\n",
       "23679    0.032700\n",
       "23683    1.870000\n",
       "23688   -0.398000\n",
       "23691   -1.430000\n",
       "23699   -0.111000\n",
       "23706   -0.541000\n",
       "23707    1.190000\n",
       "23712   -0.516000\n",
       "23713   -0.595000\n",
       "23714    0.204000\n",
       "23715   -0.606000\n",
       "23719   -0.470000\n",
       "23720    1.300000\n",
       "23724    0.058400\n",
       "23725    0.424000\n",
       "23727    0.209000\n",
       "23728   -0.785000\n",
       "23729   -1.050000\n",
       "23730    1.770000\n",
       "23737   -1.150000\n",
       "23738    0.128000\n",
       "23744    0.361000\n",
       "23745   -1.430000\n",
       "23747   -1.520000\n",
       "23748   -0.738000\n",
       "23751    0.178000\n",
       "23763    1.020000\n",
       "23767    0.030900\n",
       "23769   -0.116000\n",
       "23771    0.210000\n",
       "23772    0.146000\n",
       "23778    0.000591\n",
       "23780    0.901000\n",
       "23781    0.447000\n",
       "23787   -0.272000\n",
       "23788   -0.369000\n",
       "23799   -0.801000\n",
       "23803   -0.511000\n",
       "23805    0.824000\n",
       "23810    0.022000\n",
       "23818   -0.993000\n",
       "23819   -0.262000\n",
       "23827   -0.463000\n",
       "23830    0.531000\n",
       "23831   -1.100000\n",
       "23834   -0.833000\n",
       "23847   -0.096300\n",
       "23850    0.342000\n",
       "23853   -1.620000\n",
       "23854   -0.540000\n",
       "23855    0.286000\n",
       "23856   -0.376000\n",
       "23860   -1.320000\n",
       "23866   -0.126000\n",
       "23868   -0.459000\n",
       "23873    0.104000\n",
       "23884   -0.905000\n",
       "23886   -0.178000\n",
       "23890    1.110000\n",
       "23893   -0.051100\n",
       "23895    0.751000\n",
       "23897   -1.920000\n",
       "23899   -0.318000\n",
       "23901    0.717000\n",
       "23903   -0.520000\n",
       "23906    0.231000\n",
       "23907    0.126000\n",
       "23921    4.950000\n",
       "23932    0.944000\n",
       "23933   -0.241000\n",
       "23938   -0.485000\n",
       "23940   -0.518000\n",
       "23946    0.399000\n",
       "23951    0.939000\n",
       "23955   -1.100000\n",
       "23957   -0.568000\n",
       "23959   -0.169000\n",
       "23963   -0.008430\n",
       "23965   -0.352000\n",
       "23969   -0.997000\n",
       "23971    0.149000\n",
       "23975    0.526000\n",
       "23976    0.885000\n",
       "23978   -1.520000\n",
       "23983   -1.090000\n",
       "23994   -0.175000\n",
       "23995    0.955000\n",
       "23998    1.370000\n",
       "24000    0.414000\n",
       "24005   -0.190000\n",
       "24008   -0.993000\n",
       "24011    0.483000\n",
       "24014   -0.720000\n",
       "24016   -0.562000\n",
       "24024   -0.684000\n",
       "24031   -0.890000\n",
       "24035   -0.233000\n",
       "24042    0.067900\n",
       "24047    0.516000\n",
       "24054   -0.400000\n",
       "24057   -0.150000\n",
       "24058    0.853000\n",
       "24059   -0.578000\n",
       "24060   -0.749000\n",
       "24064    2.060000\n",
       "24072    0.850000\n",
       "24073   -0.202000\n",
       "24075    1.920000\n",
       "24080   -0.518000\n",
       "24088    0.006650\n",
       "24089   -0.504000\n",
       "24090   -1.160000\n",
       "24097   -0.600000\n",
       "24098   -0.272000\n",
       "24106    0.425000\n",
       "24111    1.630000\n",
       "24114   -0.536000\n",
       "24115    1.370000\n",
       "24122    1.570000\n",
       "24123    0.577000\n",
       "24133   -1.580000\n",
       "24135   -0.452000\n",
       "24140   -0.054300\n",
       "24141   -0.332000\n",
       "24142   -0.254000\n",
       "24144   -0.113000\n",
       "24146    0.710000\n",
       "24147    1.530000\n",
       "24148    0.294000\n",
       "24151    0.288000\n",
       "24153   -0.734000\n",
       "24154    0.446000\n",
       "24156   -0.508000\n",
       "24158    3.740000\n",
       "24165   -0.682000\n",
       "24168    0.751000\n",
       "24173   -0.476000\n",
       "24182    0.493000\n",
       "24183   -1.130000\n",
       "24187   -0.394000\n",
       "24193    1.780000\n",
       "24196   -0.842000\n",
       "24199   -0.687000\n",
       "24200   -0.886000\n",
       "24203    0.303000\n",
       "24208    2.600000\n",
       "24209    0.189000\n",
       "24217   -1.540000\n",
       "24229   -0.100000\n",
       "24230   -0.923000\n",
       "24232    1.740000\n",
       "24236   -0.747000\n",
       "24240   -0.162000\n",
       "24244   -0.146000\n",
       "24247   -0.085900\n",
       "24252    1.600000\n",
       "24253   -1.170000\n",
       "24256   -0.420000\n",
       "24261   -0.710000\n",
       "24268    0.471000\n",
       "24269   -0.160000\n",
       "24270    0.509000\n",
       "24275    0.915000\n",
       "24284   -0.886000\n",
       "24286   -0.411000\n",
       "24288    3.450000\n",
       "24293    0.944000\n",
       "24307   -0.657000\n",
       "24310   -0.189000\n",
       "24316    3.570000\n",
       "24321   -0.744000\n",
       "24322   -0.048800\n",
       "24327   -0.248000\n",
       "24328    1.500000\n",
       "24330    0.042400\n",
       "24331   -0.410000\n",
       "24332    1.580000\n",
       "24338   -0.109000\n",
       "24344    1.300000\n",
       "24351    0.116000\n",
       "24352   -0.523000\n",
       "24356   -0.087700\n",
       "24365    0.616000\n",
       "24366    0.436000\n",
       "24368   -0.137000\n",
       "24369   -0.410000\n",
       "24377   -0.708000\n",
       "24395   -1.230000\n",
       "24396    0.757000\n",
       "24397   -0.307000\n",
       "24398   -1.250000\n",
       "24403   -0.951000\n",
       "24405    0.559000\n",
       "24406   -0.399000\n",
       "24407   -0.251000\n",
       "24413   -0.687000\n",
       "24417    0.628000\n",
       "24421   -2.810000\n",
       "24428   -0.616000\n",
       "24429    0.443000\n",
       "24432    0.233000\n",
       "24437   -0.502000\n",
       "24439   -0.137000\n",
       "24441    0.054600\n",
       "24443    0.435000\n",
       "24450   -0.166000\n",
       "24462   -1.260000\n",
       "24463   -0.215000\n",
       "24466    0.144000\n",
       "24467   -1.750000\n",
       "24469   -0.747000\n",
       "24475    1.260000\n",
       "24476    0.057300\n",
       "24480    1.320000\n",
       "24496   -0.834000\n",
       "24514    0.928000\n",
       "24517   -0.923000\n",
       "24518   -1.140000\n",
       "24532    0.341000\n",
       "24533    0.244000\n",
       "24535   -0.166000\n",
       "24538    2.510000\n",
       "24543   -0.863000\n",
       "24544   -0.151000\n",
       "24545    0.483000\n",
       "24547    0.385000\n",
       "24551   -0.502000\n",
       "24554   -0.691000\n",
       "24560    1.890000\n",
       "24564    0.098600\n",
       "24565    0.717000\n",
       "24567   -1.170000\n",
       "24568   -1.240000\n",
       "24571    0.477000\n",
       "24580    0.502000\n",
       "24583   -0.040200\n",
       "24584   -0.006010\n",
       "24586   -0.051300\n",
       "24587   -0.618000\n",
       "24591   -0.877000\n",
       "24592   -1.200000\n",
       "24600    0.649000\n",
       "24604   -1.000000\n",
       "24606   -0.317000\n",
       "24608    0.866000\n",
       "24616   -0.112000\n",
       "24624   -0.233000\n",
       "24637    0.529000\n",
       "24643   -0.537000\n",
       "24644   -1.540000\n",
       "24645   -0.954000\n",
       "24651   -1.250000\n",
       "24652    1.370000\n",
       "24653   -0.018900\n",
       "24655    0.110000\n",
       "24662   -0.375000\n",
       "24670   -1.290000\n",
       "24673    3.560000\n",
       "24674   -0.361000\n",
       "24682    1.060000\n",
       "24699   -0.777000\n",
       "24700   -0.482000\n",
       "24701   -1.170000\n",
       "24702    0.001850\n",
       "24704    0.440000\n",
       "24706    0.459000\n",
       "24711    0.184000\n",
       "24712   -1.930000\n",
       "24715   -1.560000\n",
       "24719    0.425000\n",
       "24724   -0.346000\n",
       "24733   -0.149000\n",
       "24736   -0.078900\n",
       "24739   -0.333000\n",
       "24747    4.180000\n",
       "24748    1.330000\n",
       "24750   -0.006220\n",
       "24757   -0.252000\n",
       "24759    0.643000\n",
       "24763   -1.400000\n",
       "24764   -0.892000\n",
       "24765   -0.294000\n",
       "24767    0.098300\n",
       "24769   -0.037600\n",
       "24771   -0.348000\n",
       "24777    1.790000\n",
       "24780   -0.655000\n",
       "24781   -0.606000\n",
       "24783   -0.215000\n",
       "24788    0.303000\n",
       "24790    0.036300\n",
       "24792   -0.617000\n",
       "24794   -0.344000\n",
       "24798   -0.559000\n",
       "24808    0.337000\n",
       "24812   -0.967000\n",
       "24820   -0.913000\n",
       "24823   -0.655000\n",
       "24824    0.101000\n",
       "24827   -0.250000\n",
       "24834   -0.031300\n",
       "24835    2.490000\n",
       "24838    1.050000\n",
       "24839   -0.921000\n",
       "24844   -0.354000\n",
       "24845    0.365000\n",
       "24862   -1.470000\n",
       "24867   -0.619000\n",
       "24868    0.341000\n",
       "24873    2.800000\n",
       "24882   -0.207000\n",
       "24884   -0.602000\n",
       "24891   -0.246000\n",
       "24892   -0.523000\n",
       "24893    0.350000\n",
       "24895    0.320000\n",
       "24907   -0.279000\n",
       "24913   -0.736000\n",
       "24916   -0.641000\n",
       "24919   -0.138000\n",
       "24925   -0.272000\n",
       "24930   -0.151000\n",
       "24931    0.299000\n",
       "24939    3.570000\n",
       "24940   -0.410000\n",
       "24942   -0.071700\n",
       "24945   -0.183000\n",
       "24950   -1.010000\n",
       "24957   -1.070000\n",
       "24959    1.650000\n",
       "24961   -0.318000\n",
       "24965    0.376000\n",
       "24967   -0.089700\n",
       "24970   -0.249000\n",
       "24971   -0.308000\n",
       "24976   -0.188000\n",
       "24977    0.646000\n",
       "24980    0.100000\n",
       "24988   -0.634000\n",
       "24989   -0.194000\n",
       "24993   -1.930000\n",
       "24998    1.260000\n",
       "Name: 91, Length: 6125, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the data set into a training set and a testing set\n",
    "np.random.seed(9001)\n",
    "df_imp = pd.read_csv('HW6_dataset_missing.csv')\n",
    "msk = np.random.rand(len(df)) < 0.75\n",
    "data_train_imp = df_imp[msk]\n",
    "#print(data_train_imp)\n",
    "data_test_imp = df_imp[~msk]\n",
    "data_train_full = data_train_imp.dropna()\n",
    "\n",
    "data_test_imp.iloc[:, 91]\n",
    "#y_train_imp = data_train['type'].values\n",
    "#X_train_imp = data_train.values\n",
    "#y_train_imp = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "#y_test_imp = data_test_imp['type'].values\n",
    "#X_test_imp = data_test_imp.values\n",
    "#y_test_imp = y_test_imp.reshape(len(y_test), 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-d6743fd6cfd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print(X_train_imp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#print(X_train_imp.shape())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0my_train_imp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train_imp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_imp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m#print(y_train_imp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5065\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5066\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5067\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "for i in range(1,117,1):\n",
    "    y_train_imp = data_train_full.iloc[:, i]\n",
    "    #print(y_train_imp)\n",
    "    #print(y_train_imp.shape())\n",
    "    X_train_imp = data_train_full.loc[:, data_train_full.columns != i]\n",
    "    #print(X_train_imp)\n",
    "    #print(X_train_imp.shape())\n",
    "    y_train_imp = y_train_imp.reshape(len(y_train_imp), 1)\n",
    "    #print(y_train_imp)\n",
    "    \n",
    "    # regress column i on all other columns with randomness\n",
    "    regress = LinearRegression()\n",
    "    regress.fit(X_train_imp,y_train_imp)\n",
    "    y_hat = regress.predict(X_train_imp)\n",
    "    \n",
    "    X_missing = data_test_imp[data_test_imp.iloc[:, i].isnull()]\n",
    "   \n",
    "    print (X_missing)\n",
    "    if not X_missing:\n",
    "        print(\"X \", i, \"complete; nothing missing\")\n",
    "        continue\n",
    "    else:\n",
    "        print(X_missing)\n",
    "        print(TEST_missing)\n",
    "        \n",
    "\n",
    "    y_missing = regress.predict(X_missing)\n",
    "    y_missing_noise = y_missing+np.random.normal(loc=0,scale=np.sqrt(mean_squared_error(y_train_imp,y_hat)),size=y_missing.shape[0])\n",
    "\n",
    "        \n",
    "     \n",
    "    missing_index = data_train_imp.i[data_train_imp.i.isnull()].index\n",
    "    missing_series = pd.Series(data = y_missing_noise, index = missing_index)\n",
    "    \n",
    "    #back to the data set with missingness and impute the predictions\n",
    "    data_train_imp2 = data_train_imp.copy()\n",
    "    data_train_imp2[i] = data_train_imp2[i].fillna(missing_series)\n",
    "    \n",
    "    # regress on test set\n",
    "    regress.fit(X_train_imp,y_train_imp)\n",
    "    y_hat = regress.predict(X_train_imp)\n",
    "    \n",
    "    X_missing = data_train_imp[data_train_imp.i.isnull()]\n",
    "    X_missing = X_missing.reshape(len(X_missing), 1)\n",
    "    y_missing = regress.predict(X_missing)\n",
    "    y_missing_noise = y_missing+np.random.normal(loc=0,scale=np.sqrt(mean_squared_error(y_train_imp,y_hat)),size=y_missing.shape[0])\n",
    "    \n",
    "    missing_index = data_train_imp.i[data_train_imp.i.isnull()].index\n",
    "    missing_series = pd.Series(data = y_missing_noise, index = missing_index)\n",
    "    \n",
    "    #back to the data set with missingness and impute the predictions\n",
    "    data_train_imp2 = data_train_imp.copy()\n",
    "    data_train_imp2[i] = data_train_imp2[i].fillna(missing_series)\n",
    "    \n",
    "    # Fit a logistic regression classifier to the training set and report the accuracy of the classifier on the test set\n",
    "    clf = LogisticRegressionCV(\n",
    "        Cs=list(np.power(10.0, np.arange(-10, 10)))\n",
    "        ,penalty='l2'\n",
    "        ,cv=10\n",
    "        ,random_state=777\n",
    "        ,fit_intercept=True\n",
    "        ,solver='newton-cg'\n",
    "        ,tol=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # L2 Regularization parameter\n",
    "    print('\\n')\n",
    "    print(\"The optimized L2 regularization paramater id:\", clf.C_)\n",
    "\n",
    "    # The coefficients\n",
    "    print('Estimated beta1: \\n', clf.coef_)\n",
    "    print('Estimated beta0: \\n', clf.intercept_)\n",
    "\n",
    "    # Metrics\n",
    "    print('\\n')\n",
    "    print('Test Set Confusion matrix:') \n",
    "    print(confusion_matrix(y_test, clf.predict(X_test)))\n",
    "    \n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    test_score = clf.score(X_test, y_test)\n",
    "    y_prediction = clf.predict(X_test)\n",
    "    test_precision = precision_score(y_test, y_prediction)\n",
    "    print('The training classification accuracy is: ', train_score)\n",
    "    print('The testing classification accuracy is: ', test_score)\n",
    "    print('The precision score on the test set is: ', test_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "Dropping all rows with NaN\n",
    "Test Set Confusion matrix:\n",
    "[[325   0]\n",
    " [  1   0]]\n",
    "The training classification accuracy is:  0.998198198198\n",
    "The testing classification accuracy is:  0.996932515337\n",
    "The precision score on the test set is:  0.0\n",
    "\n",
    "Mean Imputation\n",
    "Test Set Confusion matrix:\n",
    "[[6074    0]\n",
    " [   3   48]]\n",
    "The training classification accuracy is:  1.0\n",
    "The testing classification accuracy is:  0.999510204082\n",
    "The precision score on the test set is:  1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the mean imputation increased classification accuracy considerably over the first model. Although the 'classification accuracy' score was very high on the first model, this is due to the very small percentage of patients who have the cancer. Given the limited data (325 remaining complete rows), there were no positive predictions. In other words, the same as our all 0 model from the previous question. This is terrible if we want to detect rare cancers. What we gain in accuracy is a tradeoff with computational complexity. The second model iterates over each column and each value computing means. The final model performs multiple regression iteratively and is by far the most computationally complex of the three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APCOMP209a - Homework Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "This problem walks you through the derivation of the **likelihood equations** for a generalized linear model (GLM). Suppose that the random component of the GLM is in the univariate natural exponential family, so that\n",
    "$$f(y_i|\\theta_i) = h(y_i) e^{y_i\\theta_i - b(\\theta_i)}$$\n",
    "Define the individual log-likelihood for each observation $i$ as\n",
    "$$l_i(\\theta_i) \\equiv \\log f(y_i|\\theta_i)$$\n",
    "with linear predictor\n",
    "$$\\eta_i = x_i^T\\beta = g(\\mu_i)$$\n",
    "for some link function $g$ and where $\\mu_i=E(Y_i)$.\n",
    "\n",
    "1. Use the above expressions to write a simplified expression for the log-likelihood $l(\\theta)$ for the entire dataset, $y_1, \\dots, y_n$.\n",
    "\n",
    "2. Use the chain rule to express $\\frac{\\partial l_i}{\\partial \\beta_j}$ in terms of the derivatives of $l_i, \\theta_i, \\mu_i$, and $\\eta_i$. (*Hint*: Think carefully about which variables are related to which, and in what way. For example, for which of the above variables do you know the derivative with respect to $\\beta_j$?)\n",
    "\n",
    "3. Compute the derivatives for $\\frac{\\partial l_i}{\\partial \\theta_i}$ and $\\frac{\\partial \\eta_i}{\\partial \\beta_j}$.\n",
    "\n",
    "4. Express $\\mu_i$ in terms of $\\theta_i$, and use this relationship to compute $\\frac{\\partial \\theta_i}{\\partial \\mu_i}$. (\\emph{Hint}: Recall the cumulant function of a natural exponential family, and assume that you can write $\\partial f/\\partial g = (\\partial g / \\partial f)^{-1}$.)\n",
    "\n",
    "5. Express $\\eta_i$ in terms of $\\mu_i$. Using the same hint as the above, compute $\\frac{\\partial \\mu_i}{\\partial \\eta_i}$.\n",
    "\n",
    "6. Put all of the above parts together to write an expression for $\\frac{\\partial l}{\\partial \\beta_j}$. Use matrix notation to write this expression as\n",
    "$$\\nabla_{\\beta} l(\\beta) = XDV^{-1}(Y - \\mu) = 0$$\n",
    "That is, compute the matrices $D$ and $V$ such that this equation holds.\n",
    "\n",
    "7. If we use the canonical link function, how do your answers to part (6) simplify?\n",
    "\n",
    "8. Finally, compute the above likelihood equations in the case of logistic regression, and show that this is equivalent to the solution given in lecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
